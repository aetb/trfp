{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "* The `summed_azimuth` arguments are wrong, currently only using the first two trolley runs. This is probably fine for now, but definitely needs to be fixed in a better framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/04\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy\n",
    "\n",
    "import gm2\n",
    "import trfp\n",
    "import plotting_functions as plt2\n",
    "import analysis_helper as helper\n",
    "import helper_function_candidates as helper_old\n",
    "\n",
    "import mu_avg.mu_avg as mu_avg\n",
    "\n",
    "blinds = np.loadtxt('blinds.txt')\n",
    "\n",
    "def apply_blinds_fp(input_df, blinds):\n",
    "    output_df = input_df.copy()\n",
    "    for m in range(6):\n",
    "        stms = ['st'+str(st)+',m'+str(m+1) for st in range(72)]\n",
    "        output_df[stms] = output_df[stms] + blinds[m]\n",
    "    return output_df\n",
    "\n",
    "def apply_blinds_tr(input_df, blinds):\n",
    "    output_df = input_df.copy()\n",
    "    for m in range(6):\n",
    "        stms = ['st'+str(st)+',m'+str(m+1) for st in range(72)]\n",
    "        output_df[stms] = output_df[stms] + blinds[m]\n",
    "        trms = ['tr,m'+str(m+1)]\n",
    "        output_df[trms] = output_df[trms] + blinds[m]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 60hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename = 'hdf5/60hr.h5'\n",
    "\n",
    "fp_interp_df = pd.read_hdf(filename, key='fp_df_1')\n",
    "tr_interp_df_1 = pd.read_hdf(filename, key='tr_df_1')\n",
    "tr_interp_df_2 = pd.read_hdf(filename, key='tr_df_2')\n",
    "subrun_df = pd.read_hdf(filename, key='subrun_df')\n",
    "\n",
    "fp_moment_df = helper.calc_moment_df(fp_interp_df)\n",
    "tr_moment_df_1 = helper.calc_moment_df(tr_interp_df_1)\n",
    "tr_moment_df_2 = helper.calc_moment_df(tr_interp_df_2)\n",
    "\n",
    "tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "\n",
    "vtm_df = helper.vtm_calc(fp_moment_df,\n",
    "                         baseline_time_1, baseline_time_2,\n",
    "                         tr_baseline_1, tr_baseline_2,\n",
    "                         fp_baseline_1, fp_baseline_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 9day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "filename = 'hdf5/9day.h5'\n",
    "\n",
    "subrun_df = pd.read_hdf(filename, key='subrun_df')\n",
    "\n",
    "print 'fp run 1'\n",
    "fp_moment_df_1 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_1')), blinds)\n",
    "print 'fp run 2'\n",
    "fp_moment_df_2 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_2')), blinds)\n",
    "print 'fp run 3'\n",
    "fp_moment_df_3 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_3')), blinds)\n",
    "print 'fp run 4'\n",
    "fp_moment_df_4 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_4')), blinds)\n",
    "\n",
    "print 'tr run 1'\n",
    "tr_moment_df_1 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_1')), blinds)\n",
    "print 'tr run 2'\n",
    "tr_moment_df_2 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_2')), blinds)\n",
    "print 'tr run 3'\n",
    "tr_moment_df_3 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_3')), blinds)\n",
    "print 'tr run 4'\n",
    "tr_moment_df_4 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_4')), blinds)\n",
    "print 'tr run 5'\n",
    "tr_moment_df_5 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_5')), blinds)\n",
    "print 'tr run 6'\n",
    "tr_moment_df_6 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_6')), blinds)\n",
    "\n",
    "tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "tr_corr_df_3 = helper_old.trolley_footprint_replacement(tr_moment_df_3)\n",
    "tr_corr_df_4 = helper_old.trolley_footprint_replacement(tr_moment_df_4)\n",
    "tr_corr_df_5 = helper_old.trolley_footprint_replacement(tr_moment_df_5)\n",
    "tr_corr_df_6 = helper_old.trolley_footprint_replacement(tr_moment_df_6)\n",
    "\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "tr_baseline_3, fp_baseline_3, baseline_time_3, summed_azimuth_3, summed_pts_3 = helper_old.trolley_run_station_average(tr_corr_df_3)\n",
    "tr_baseline_4, fp_baseline_4, baseline_time_4, summed_azimuth_4, summed_pts_4 = helper_old.trolley_run_station_average(tr_corr_df_4)\n",
    "tr_baseline_5, fp_baseline_5, baseline_time_5, summed_azimuth_5, summed_pts_5 = helper_old.trolley_run_station_average(tr_corr_df_5)\n",
    "tr_baseline_6, fp_baseline_6, baseline_time_6, summed_azimuth_6, summed_pts_6 = helper_old.trolley_run_station_average(tr_corr_df_6)\n",
    "\n",
    "vtm_df = helper.vtm_calc(fp_moment_df_1,\n",
    "                         baseline_time_1, baseline_time_2,\n",
    "                         tr_baseline_1, tr_baseline_2,\n",
    "                         fp_baseline_1, fp_baseline_2)\n",
    "\n",
    "vtm_df = vtm_df.append(helper.vtm_calc(fp_moment_df_2,\n",
    "                                       baseline_time_3, baseline_time_4,\n",
    "                                       tr_baseline_3, tr_baseline_4,\n",
    "                                       fp_baseline_3, fp_baseline_4))\n",
    "\n",
    "vtm_df = vtm_df.append(helper.vtm_calc(fp_moment_df_3,\n",
    "                                       baseline_time_4, baseline_time_5,\n",
    "                                       tr_baseline_4, tr_baseline_5,\n",
    "                                       fp_baseline_4, fp_baseline_5))\n",
    "\n",
    "vtm_df = vtm_df.append(helper.vtm_calc(fp_moment_df_4,\n",
    "                                       baseline_time_5, baseline_time_6,\n",
    "                                       tr_baseline_6, tr_baseline_6,\n",
    "                                       fp_baseline_6, fp_baseline_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### High Kick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename = 'hdf5/highkick.h5'\n",
    "\n",
    "\n",
    "subrun_df = pd.read_hdf(filename, key='subrun_df')\n",
    "\n",
    "print 'fp run 1'\n",
    "fp_moment_df_1 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_1')), blinds)\n",
    "print 'fp run 2'\n",
    "fp_moment_df_2 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_2')), blinds)\n",
    "\n",
    "print 'tr run 1'\n",
    "tr_moment_df_1 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_1')), blinds)\n",
    "print 'tr run 2'\n",
    "tr_moment_df_2 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_2')), blinds)\n",
    "print 'tr run 3'\n",
    "tr_moment_df_3 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_3')), blinds)\n",
    "\n",
    "tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "tr_corr_df_3 = helper_old.trolley_footprint_replacement(tr_moment_df_3)\n",
    "\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "tr_baseline_3, fp_baseline_3, baseline_time_3, summed_azimuth_3, summed_pts_3 = helper_old.trolley_run_station_average(tr_corr_df_3)\n",
    "\n",
    "vtm_df = helper.vtm_calc(fp_moment_df_1,\n",
    "                         baseline_time_1, baseline_time_2,\n",
    "                         tr_baseline_1, tr_baseline_2,\n",
    "                         fp_baseline_1, fp_baseline_2)\n",
    "\n",
    "vtm_df = vtm_df.append(helper.vtm_calc(fp_moment_df_2,\n",
    "                                       baseline_time_2, baseline_time_3,\n",
    "                                       tr_baseline_2, tr_baseline_3,\n",
    "                                       fp_baseline_2, fp_baseline_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endgame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp run 2\n",
      "Finished calculating all moments for 307497 events.         \n",
      "fp run 3\n",
      "Finished calculating all moments for 251483 events. \n",
      "fp run 4\n",
      "Finished calculating all moments for 310474 events.         \n",
      "fp run 5\n",
      "Finished calculating all moments for 268591 events.  \n",
      "fp run 6\n",
      "Finished calculating all moments for 280860 events.   \n",
      "tr run 2\n",
      "Finished calculating all moments for 4914 events.\n",
      "tr run 3\n",
      "Finished calculating all moments for 4391 events.\n",
      "tr run 4\n",
      "Finished calculating all moments for 4350 events.\n",
      "tr run 5\n",
      "Finished calculating all moments for 4371 events.\n",
      "tr run 6\n",
      "Finished calculating all moments for 4345 events.\n",
      "tr run 7\n",
      "Finished calculating all moments for 4350 events.\n",
      "tr run 8\n",
      "Finished calculating all moments for 4394 events.\n",
      "tr run 9\n",
      "Finished calculating all moments for 3265 events.\n",
      "Removing trolley image from station 71.                                                                                            \n"
     ]
    }
   ],
   "source": [
    "subrun_df = pd.read_hdf('hdf5/endgame.h5', key='subrun_df')\n",
    "\n",
    "# print 'fp run 1'\n",
    "# fp_moment_df_1 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'fp_df_1')), blinds)\n",
    "print 'fp run 2'\n",
    "fp_moment_df_2 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'fp_df_2')), blinds)\n",
    "print 'fp run 3'\n",
    "fp_moment_df_3 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'fp_df_3')), blinds)\n",
    "print 'fp run 4'\n",
    "fp_moment_df_4 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'fp_df_4')), blinds)\n",
    "print 'fp run 5'\n",
    "fp_moment_df_5 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'fp_df_5')), blinds)\n",
    "print 'fp run 6'\n",
    "fp_moment_df_6 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'fp_df_6')), blinds)\n",
    "\n",
    "# print 'tr run 1'\n",
    "# tr_moment_df_1 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_1')), blinds)\n",
    "print 'tr run 2'\n",
    "tr_moment_df_2 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_2')), blinds)\n",
    "print 'tr run 3'\n",
    "tr_moment_df_3 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_3')), blinds)\n",
    "print 'tr run 4'\n",
    "tr_moment_df_4 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_4')), blinds)\n",
    "print 'tr run 5'\n",
    "tr_moment_df_5 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_5')), blinds)\n",
    "print 'tr run 6'\n",
    "tr_moment_df_6 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_6')), blinds)\n",
    "print 'tr run 7'\n",
    "tr_moment_df_7 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_7')), blinds)\n",
    "print 'tr run 8'\n",
    "tr_moment_df_8 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_8')), blinds)\n",
    "print 'tr run 9'\n",
    "tr_moment_df_9 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/endgame.h5', 'tr_df_9')), blinds)\n",
    "\n",
    "## FIRST TROLLEY PAIR IS DROPPED\n",
    "\n",
    "# tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "tr_corr_df_3 = helper_old.trolley_footprint_replacement(tr_moment_df_3)\n",
    "tr_corr_df_4 = helper_old.trolley_footprint_replacement(tr_moment_df_4)\n",
    "tr_corr_df_5 = helper_old.trolley_footprint_replacement(tr_moment_df_5)\n",
    "tr_corr_df_6 = helper_old.trolley_footprint_replacement(tr_moment_df_6)\n",
    "tr_corr_df_7 = helper_old.trolley_footprint_replacement(tr_moment_df_7)\n",
    "tr_corr_df_8 = helper_old.trolley_footprint_replacement(tr_moment_df_8)\n",
    "# tr_corr_df_9 = helper_old.trolley_footprint_replacement(tr_moment_df_9)\n",
    "\n",
    "# tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "tr_baseline_3, fp_baseline_3, baseline_time_3, summed_azimuth_3, summed_pts_3 = helper_old.trolley_run_station_average(tr_corr_df_3)\n",
    "\n",
    "tr_baseline_4, fp_baseline_4, baseline_time_4, summed_azimuth_4, summed_pts_4 = helper_old.trolley_run_station_average(tr_corr_df_4)\n",
    "tr_baseline_5, fp_baseline_5, baseline_time_5, summed_azimuth_5, summed_pts_5 = helper_old.trolley_run_station_average(tr_corr_df_5)\n",
    "tr_baseline_6, fp_baseline_6, baseline_time_6, summed_azimuth_6, summed_pts_6 = helper_old.trolley_run_station_average(tr_corr_df_6)\n",
    "\n",
    "tr_baseline_7, fp_baseline_7, baseline_time_7, summed_azimuth_7, summed_pts_7 = helper_old.trolley_run_station_average(tr_corr_df_7)\n",
    "tr_baseline_8, fp_baseline_8, baseline_time_8, summed_azimuth_8, summed_pts_8 = helper_old.trolley_run_station_average(tr_corr_df_8)\n",
    "# tr_baseline_9, fp_baseline_9, baseline_time_9, summed_azimuth_9, summed_pts_9 = helper_old.trolley_run_station_average(tr_corr_df_9)\n",
    "\n",
    "## vtm_df_1 will have weirdness due to SCC shenanigans\n",
    "# vtm_df_1 = helper.vtm_calc(fp_moment_df_1,\n",
    "#                            baseline_time_1, baseline_time_2,  ## only backwards track?\n",
    "#                            tr_baseline_2, tr_baseline_2,\n",
    "#                            fp_baseline_2, fp_baseline_2)\n",
    "\n",
    "vtm_df = helper.vtm_calc(fp_moment_df_2,\n",
    "                         baseline_time_2, baseline_time_3,\n",
    "                         tr_baseline_2, tr_baseline_3,\n",
    "                         fp_baseline_2, fp_baseline_3)\n",
    "\n",
    "vtm_df = vtm_df.append(helper.vtm_calc(fp_moment_df_3,\n",
    "                                       baseline_time_4, baseline_time_5,\n",
    "                                       tr_baseline_4, tr_baseline_5,\n",
    "                                       fp_baseline_4, fp_baseline_5))\n",
    "\n",
    "vtm_df = vtm_df.append(helper.vtm_calc(fp_moment_df_4,\n",
    "                                       baseline_time_6, baseline_time_7,\n",
    "                                       tr_baseline_6, tr_baseline_7,\n",
    "                                       fp_baseline_6, fp_baseline_7))\n",
    "\n",
    "vtm_df = vtm_df.append(helper.vtm_calc(fp_moment_df_5,\n",
    "                                       baseline_time_7, baseline_time_8,\n",
    "                                       tr_baseline_7, tr_baseline_8,\n",
    "                                       fp_baseline_7, fp_baseline_8))\n",
    "\n",
    "vtm_df = vtm_df.append(helper.vtm_calc(fp_moment_df_6,\n",
    "                                       baseline_time_8, baseline_time_7,  # NO BACKWARDS TRACKING\n",
    "                                       tr_baseline_8, tr_baseline_8,\n",
    "                                       fp_baseline_8, fp_baseline_8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_numbers = np.unique(subrun_df['run'])\n",
    "runs = {}\n",
    "for run in run_numbers:\n",
    "    runs[run] = [int(np.min(subrun_df[subrun_df['run']==run]['start_gps'])),\n",
    "                 int(np.max(subrun_df[subrun_df['run']==run]['end_gps']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_run_df = pd.DataFrame(index = run_numbers, columns = ['B', 'dB'])\n",
    "naive_by_run_df = pd.DataFrame(index = run_numbers, columns = ['B', 'dB'])\n",
    "\n",
    "for run in run_numbers:\n",
    "    intervals = []\n",
    "    \n",
    "    mini_subrun_df = subrun_df[subrun_df['run']==run].copy()\n",
    "    \n",
    "    run_start = np.min(mini_subrun_df['start_time'])\n",
    "    run_end = np.max(mini_subrun_df['end_time'])\n",
    "\n",
    "#     for ii in range(subrun_df[subrun_df['run']==run].index.values.size):\n",
    "#         intervals.append(pd.Interval(mini_subrun_df['start_time'].values[ii],\n",
    "#                                      mini_subrun_df['end_time'].values[ii],\n",
    "#                                      closed='both'\n",
    "#                                     )\n",
    "#                         )\n",
    "#     intervals = pd.IntervalIndex(intervals, closed='both')\n",
    "\n",
    "    for ii in range(subrun_df[subrun_df['run']==run].index.values.size):\n",
    "        intervals.append(mini_subrun_df['start_time'].values[ii])\n",
    "    intervals.append(mini_subrun_df['end_time'].values[-1])\n",
    "    \n",
    "    mini_vtm_df = vtm_df[(vtm_df.index > run_start) & (vtm_df.index < run_end)]\n",
    "\n",
    "    vtm_bin_df = mini_vtm_df.groupby(pd.cut(mini_vtm_df.index, intervals)).mean()\n",
    "    vtm_bin_df['ctags'] = subrun_df[subrun_df['run']==run]['ctags'].values\n",
    "\n",
    "    stms = ['st'+str(st)+',m'+str(m+1) for st in range(72) for m in range(6)]\n",
    "    time_avg_series = vtm_bin_df[stms].multiply(vtm_bin_df['ctags'], axis='index').sum()/vtm_bin_df['ctags'].sum()\n",
    "\n",
    "    azi_avg_series = pd.Series(index = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "    for m in range(5):\n",
    "        weight = summed_azimuth_2[:, m] + summed_azimuth_3[:, m]\n",
    "\n",
    "        total_weight = np.nansum(weight)\n",
    "        stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "        azi_avg_series['m'+str(m+1)] = time_avg_series[stm_list].multiply(weight).sum()/total_weight\n",
    "\n",
    "    by_run_df['B'].loc[run] = azi_avg_series['m1'].item()\n",
    "    by_run_df['dB'].loc[run] = 6\n",
    "    \n",
    "#     naive_time_avg_series = vtm_bin_df[stms].mean()\n",
    "#     naive_azi_avg_series = pd.Series(index = ['m' + str(m) for m in np.arange(6)+1])\n",
    "#     for m in range(5):\n",
    "#         weight = summed_azimuth_1[:, m] + summed_azimuth_2[:, m]\n",
    "\n",
    "#         total_weight = np.nansum(weight)\n",
    "#         stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "#         naive_azi_avg_series['m'+str(m+1)] = naive_time_avg_series[stm_list].multiply(weight).sum()/total_weight\n",
    "        \n",
    "#     naive_by_run_df['B'].loc[run] = naive_azi_avg_series['m1']\n",
    "#     naive_by_run_df['dB'].loc[run] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_run_df['B'] = pd.to_numeric(by_run_df['B'])\n",
    "by_run_df = by_run_df[by_run_df['B'] != 0.0]\n",
    "by_run_df.to_csv('endgame_naive_muon_avg.csv', index_label='run', float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
