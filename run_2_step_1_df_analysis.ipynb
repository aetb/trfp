{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import gm2\n",
    "import trfp\n",
    "import plotting_functions as plt2\n",
    "# import analysis_helper as helper\n",
    "# import helper_function_candidates as helper_old\n",
    "# import df_analysis_funcs as df_func\n",
    "import helper_functions as helper\n",
    "\n",
    "import field_map_config_run2 as map_config\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "blinds = np.loadtxt('blinds.txt')\n",
    "pair_dict = map_config.pair_dict\n",
    "\n",
    "def azi_avg(moment_df):\n",
    "    \n",
    "    weight = np.diff(trfp.geometry.STATION_BARCODE_EDGES)\n",
    "    weight[2] = weight[2]+360.\n",
    "    weight = weight/360.\n",
    "    \n",
    "    avg_df = pd.DataFrame(index=moment_df.index)\n",
    "    for m in range(9):\n",
    "        stms = ['st'+str(st)+',m'+str(m+1) for st in range(72)]\n",
    "        avg_df['m'+str(m+1)] = moment_df[stms].multiply(weight).sum(axis=1)\n",
    "    return avg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving run_2b1\n",
      "Saving run_2b2\n",
      "\n",
      "Reading fp_df_1\n",
      "\n",
      "Reading fp_df_2\n",
      "\n",
      "Reading fp_df_3\n",
      "\n",
      "Reading fp_df_4\n",
      "\n",
      "Reading tr_df_1\n",
      "\n",
      "Reading tr_df_2\n",
      "\n",
      "Reading tr_df_3a\n",
      "\n",
      "Reading tr_df_3b\n",
      "\n",
      "Reading tr_df_4\n",
      "\n",
      "Reading tr_df_5\n",
      "\n",
      "Calculating moments for fp_df_1\n",
      "Finished calculating all moments for 361491 events.          \n",
      "\n",
      "Calculating moments for fp_df_2\n",
      "Finished calculating all moments for 260932 events.     \n",
      "\n",
      "Calculating moments for fp_df_3\n",
      "Finished calculating all moments for 263196 events.    \n",
      "\n",
      "Calculating moments for fp_df_4\n",
      "Finished calculating all moments for 197469 events.\n",
      "\n",
      "Calculating moments for tr_df_1\n",
      "Finished calculating all moments for 4366 events.\n",
      "\n",
      "Calculating moments for tr_df_2\n",
      "Finished calculating all moments for 4319 events.\n",
      "\n",
      "Calculating moments for tr_df_3a\n",
      "Finished calculating all moments for 208 events.\n",
      "\n",
      "Calculating moments for tr_df_3b\n",
      "Finished calculating all moments for 4365 events.\n",
      "\n",
      "Calculating moments for tr_df_4\n",
      "Finished calculating all moments for 4389 events.\n",
      "\n",
      "Calculating moments for tr_df_5\n",
      "Finished calculating all moments for 4428 events.\n",
      "\n",
      "Blinding fp_df_1\n",
      "\n",
      "Blinding fp_df_2\n",
      "\n",
      "Blinding fp_df_3\n",
      "\n",
      "Blinding fp_df_4\n",
      "\n",
      "Blinding tr_df_1\n",
      "\n",
      "Blinding tr_df_2\n",
      "\n",
      "Blinding tr_df_3a\n",
      "\n",
      "Blinding tr_df_3b\n",
      "\n",
      "Blinding tr_df_4\n",
      "\n",
      "Blinding tr_df_5\n",
      "\n",
      "Removing trolley footprints for tr_df_1\n",
      "Removing trolley image from station 71.              \n",
      "Removing trolley footprints for tr_df_2\n",
      "Removing trolley image from station 71.              \n",
      "Removing trolley footprints for tr_df_3a\n",
      "Removing trolley image from station 0."
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected non-empty vector for x",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0a11ee0ffd93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmoment_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_to_moment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterp_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmoment_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblind_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoment_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcorrected_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoment_to_corrected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoment_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     baselines = helper.station_average(corrected_dfs, keys)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aetb/pythonScripts/trfp/helper_functions.pyc\u001b[0m in \u001b[0;36mmoment_to_corrected\u001b[0;34m(moment_dfs, keys)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\nRemoving trolley footprints for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0mcorrected_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrolley_footprint_replacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoment_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcorrected_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoment_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aetb/pythonScripts/trfp/helper_functions.pyc\u001b[0m in \u001b[0;36mtrolley_footprint_replacement\u001b[0;34m(tr_moment_df, veto_extent)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_mask_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mveto_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0mlocal_drift_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mlocal_drift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_drift_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_mask_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/numpy/lib/polynomial.pyc\u001b[0m in \u001b[0;36mpolyfit\u001b[0;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected 1D vector for x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected non-empty vector for x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected 1D or 2D array for y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected non-empty vector for x"
     ]
    }
   ],
   "source": [
    "runs = map_config.runs\n",
    "\n",
    "columns = [['st'+str(st)+',dt'] +  ['st'+str(st)+',m'+str(m+1) for m in range(9)] for st in range(72)]\n",
    "columns = [item for sublist in columns for item in sublist]\n",
    "sync_offset_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for run in runs:\n",
    "    filename = 'hdf5/2021-01-11_run_' + run + '.h5'\n",
    "\n",
    "    interp_dfs, keys, subrun_df = helper.read_dfs(filename)\n",
    "    run_pair_dict = pair_dict[run]\n",
    "\n",
    "    moment_dfs = helper.interp_to_moment(interp_dfs, keys)\n",
    "    moment_dfs = helper.blind_moments(moment_dfs, keys, blinds)\n",
    "    corrected_dfs = helper.moment_to_corrected(moment_dfs, keys)\n",
    "\n",
    "#     baselines = helper.station_average(corrected_dfs, keys)\n",
    "\n",
    "    ## for endgame only\n",
    "    # for item in baselines:\n",
    "    #     baselines[item]['tr_df_9'] = baselines[item]['tr_df_8']\n",
    "    # baselines['time']['tr_df_9'] = baselines['time']['tr_df_7']\n",
    "\n",
    "#     vtm_dfs = helper.calculate_vtms(corrected_dfs, keys, baselines, run_pair_dict)\n",
    "\n",
    "    bloch_style_dfs = helper.bloch_style_moments(corrected_dfs, keys)\n",
    "    baselines_bloch = helper.station_average(bloch_style_dfs, keys)\n",
    "\n",
    "    ## for endgame only\n",
    "    # for item in baselines_bloch:\n",
    "    #     baselines_bloch[item]['tr_df_9'] = baselines_bloch[item]['tr_df_8']\n",
    "    # baselines_bloch['time']['tr_df_9'] = baselines_bloch['time']['tr_df_7']\n",
    "\n",
    "    vtm_dfs_bloch = helper.calculate_vtms(bloch_style_dfs, keys, baselines_bloch, run_pair_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Sync Offset\n",
    "\n",
    "    \n",
    "    \n",
    "    for pair in pair_dict[run]:\n",
    "        tr_bl_1 = baselines_bloch['tr'][pair_dict[run][pair][0]]\n",
    "        tr_bl_2 = baselines_bloch['tr'][pair_dict[run][pair][1]]\n",
    "        fp_bl_1 = baselines_bloch['fp'][pair_dict[run][pair][0]]\n",
    "        fp_bl_2 = baselines_bloch['fp'][pair_dict[run][pair][1]]\n",
    "        time_bl_1 = baselines_bloch['time'][pair_dict[run][pair][0]]\n",
    "        time_bl_2 = baselines_bloch['time'][pair_dict[run][pair][1]]\n",
    "        \n",
    "        delta_tr = tr_bl_2 - tr_bl_1\n",
    "        delta_fp = fp_bl_2 - fp_bl_1\n",
    "        delta_time = time_bl_2 - time_bl_1\n",
    "        \n",
    "        sync_offsets = np.empty((72,9))\n",
    "        for st in range(72):\n",
    "            J = helper._choose_J(st)\n",
    "            sync_offsets[st,0:5] = delta_tr[st,0:5] -  np.matmul(J, delta_fp[st,0:5])\n",
    "            sync_offsets[st,5:9] = delta_tr[st,5:9]\n",
    "        \n",
    "        row = {}\n",
    "        for st in range(72):\n",
    "            for m in range(9):\n",
    "                row['st'+str(st)+',m'+str(m+1)] = sync_offsets[st,m]\n",
    "            row['st'+str(st)+',dt'] = delta_time[st]\n",
    "\n",
    "        sync_offset_df = sync_offset_df.append(pd.DataFrame(row, index=['run_'+run+pair[6]]))\n",
    "    \n",
    "    \n",
    "    ## Done w/ sync offsets\n",
    "\n",
    "    clear_output()\n",
    "    \n",
    "    for key in vtm_dfs_bloch.keys():\n",
    "#         a = vtm_dfs[key].index.values\n",
    "#         bins = np.array([a,a]).T.flatten()[:len(a)]\n",
    "\n",
    "#         vtm_2_sec_df = vtm_dfs_bloch[key].groupby(bins).mean()\n",
    "#         vtm_2_sec_df = vtm_2_sec_df.set_index((a[::2]+0.5))\n",
    "\n",
    "#         weight = trfp.STATION_BARCODE_EDGES[1:] - trfp.STATION_BARCODE_EDGES[:-1]\n",
    "#         weight[2] = weight[2] + 360\n",
    "\n",
    "#         vtm_2_sec_azi_avg_df = pd.DataFrame(index=vtm_2_sec_df.index)\n",
    "\n",
    "#         for m in range(9):\n",
    "#             stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "#             vtm_2_sec_azi_avg_df['m'+str(m+1)] = vtm_2_sec_df[stm_list].multiply(weight).sum(axis=1)/360\n",
    "            \n",
    "        # need to save 4 data frames\n",
    "\n",
    "        save_key = 'run_' + run + key[6]\n",
    "        save_dir = '/data2/aetb/'\n",
    "        save_prefix = '2021-03-17_run-2_'\n",
    "        \n",
    "        print 'Saving ' + save_key\n",
    "\n",
    "        # save purcell style\n",
    "\n",
    "#         save_path = save_dir + save_prefix + 'purcell_maps.h5'\n",
    "#         vtm_dfs[key].to_hdf(save_path, key=save_key)\n",
    "\n",
    "        # save hybrid\n",
    "\n",
    "        save_path = save_dir + save_prefix + 'hybrid_maps.h5'\n",
    "        vtm_dfs_bloch[key].to_hdf(save_path, key=save_key)\n",
    "\n",
    "        # save 2 sec hybrid\n",
    "\n",
    "#         save_path = save_dir + save_prefix + 'hybrid_maps_2sec.h5'\n",
    "#         vtm_2_sec_df.to_hdf(save_path, key=save_key)\n",
    "\n",
    "        # save 2 sec azi avg hybrid\n",
    "\n",
    "#         save_path = save_dir + save_prefix + 'hybrid_maps_2sec_aziavg.h5'\n",
    "#         vtm_2_sec_azi_avg_df.to_hdf(save_path, key=save_key)\n",
    "\n",
    "print 'Saving Sync Offsets'\n",
    "\n",
    "save_path = save_dir + save_prefix + 'hybrid_syncs.h5'\n",
    "save_key = 'sync_offset_df'\n",
    "sync_offset_df.to_hdf(save_path, key=save_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_offset_df.head()\n",
    "plt.hist(sync_offset_df[['st'+str(st)+',m1' for st in range(72)]].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bl_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_offset_calc(tr_corr_df_1, tr_corr_df_2):\n",
    "    tr_baseline_1, fp_baseline_1, baseline_time_1, _, _ = trolley_run_station_average(tr_corr_df_1)\n",
    "    tr_baseline_2, fp_baseline_2, baseline_time_2, _, _ = trolley_run_station_average(tr_corr_df_2)\n",
    "    delta_time = baseline_time_2 - baseline_time_1\n",
    "    \n",
    "    delta_tr_baseline = tr_baseline_2 - tr_baseline_1\n",
    "    delta_fp_baseline = fp_baseline_2 - fp_baseline_1\n",
    "    \n",
    "    sync_offsets = np.empty((72,9))\n",
    "    for st in range(72):\n",
    "        J = _choose_J(st)\n",
    "        sync_offsets[st,0:5] = delta_tr_baseline[st,0:5] -  np.matmul(J, delta_fp_baseline[st,0:5])\n",
    "        sync_offsets[st,5:9] = delta_tr_baseline[st,5:9]\n",
    "    \n",
    "    return sync_offsets, delta_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data2/aetb/2021-03-17_hybrid_maps.h5', key='run_2a1')\n",
    "avg_df = azi_avg(df)\n",
    "plt.plot(avg_df.index.values, avg_df['m2'], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data2/aetb/2021-03-17_hybrid_maps.h5', key='run_2b1')\n",
    "avg_df = azi_avg(df)\n",
    "plt.plot(avg_df.index.values, avg_df['m2'], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data2/aetb/2021-03-17_hybrid_maps.h5', key='run_2b2')\n",
    "avg_df = azi_avg(df)\n",
    "plt.plot(avg_df.index.values, avg_df['m2'], '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## looking into reducing df sizes\n",
    "\n",
    "avg_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = avg_df['m1'].iloc[0]\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x\n",
    "print x.astype(np.float64)\n",
    "print x.astype(np.float32)\n",
    "print x.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x + 61.79e6).astype(np.float64)\n",
    "print (x + 61.79e6).astype(np.float32)\n",
    "print (x + 61.79e6).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.binary_repr(61840884)\n",
    "print np.binary_repr(61840884)\n",
    "print len(np.binary_repr(61840884))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
