{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General data set analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/04\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import gm2\n",
    "import trfp\n",
    "import plotting_functions as plt2\n",
    "import analysis_helper as helper\n",
    "import helper_function_candidates as helper_old\n",
    "\n",
    "blinds = np.loadtxt('blinds.txt')\n",
    "\n",
    "def apply_blinds_fp(input_df, blinds):\n",
    "    output_df = input_df.copy()\n",
    "    for m in range(6):\n",
    "        stms = ['st'+str(st)+',m'+str(m+1) for st in range(72)]\n",
    "        output_df[stms] = output_df[stms] + blinds[m]\n",
    "    return output_df\n",
    "\n",
    "def apply_blinds_tr(input_df, blinds):\n",
    "    output_df = input_df.copy()\n",
    "    for m in range(6):\n",
    "        stms = ['st'+str(st)+',m'+str(m+1) for st in range(72)]\n",
    "        output_df[stms] = output_df[stms] + blinds[m]\n",
    "        trms = ['tr,m'+str(m+1)]\n",
    "        output_df[trms] = output_df[trms] + blinds[m]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60 hr Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished calculating all moments for 257281 events.\n",
      "Finished calculating all moments for 4386 events.\n",
      "Finished calculating all moments for 4363 events.\n",
      "Removing trolley image from station 71.                            \n"
     ]
    }
   ],
   "source": [
    "filename = 'hdf5/60hr_v920.h5'\n",
    "\n",
    "fp_interp_df = pd.read_hdf(filename, key='fp_df_1')\n",
    "tr_interp_df_1 = pd.read_hdf(filename, key='tr_df_1')\n",
    "tr_interp_df_2 = pd.read_hdf(filename, key='tr_df_2')\n",
    "\n",
    "fp_moment_df = helper.calc_moment_df(fp_interp_df)\n",
    "tr_moment_df_1 = helper.calc_moment_df(tr_interp_df_1)\n",
    "tr_moment_df_2 = helper.calc_moment_df(tr_interp_df_2)\n",
    "\n",
    "tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "\n",
    "vtm_df = helper.vtm_calc(fp_moment_df,\n",
    "                         baseline_time_1, baseline_time_2,\n",
    "                         tr_baseline_1, tr_baseline_2,\n",
    "                         fp_baseline_1, fp_baseline_2)\n",
    "\n",
    "vtm_df.to_hdf('hdf5/60hr_moments_v920.h5', key='vtm_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(_vtm_df['st21,m1'], '.')\n",
    "plt.plot(vtm_df['st21,m1'], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_offsets, delta_time = helper.sync_offset_calc(tr_corr_df_1, tr_corr_df_2)\n",
    "\n",
    "plt.plot(sync_offsets[:,0], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'hdf5/60hr.h5'\n",
    "\n",
    "_fp_interp_df = pd.read_hdf(filename, key='fp_df_1')\n",
    "_tr_interp_df_1 = pd.read_hdf(filename, key='tr_df_1')\n",
    "_tr_interp_df_2 = pd.read_hdf(filename, key='tr_df_2')\n",
    "\n",
    "_fp_moment_df = helper.calc_moment_df(_fp_interp_df)\n",
    "_tr_moment_df_1 = helper.calc_moment_df(_tr_interp_df_1)\n",
    "_tr_moment_df_2 = helper.calc_moment_df(_tr_interp_df_2)\n",
    "\n",
    "_tr_corr_df_1 = helper_old.trolley_footprint_replacement(_tr_moment_df_1)\n",
    "_tr_corr_df_2 = helper_old.trolley_footprint_replacement(_tr_moment_df_2)\n",
    "\n",
    "_tr_baseline_1, _fp_baseline_1, _baseline_time_1, _summed_azimuth_1, _summed_pts_1 = helper_old.trolley_run_station_average(_tr_corr_df_1)\n",
    "_tr_baseline_2, _fp_baseline_2, _baseline_time_2, _summed_azimuth_2, _summed_pts_2 = helper_old.trolley_run_station_average(_tr_corr_df_2)\n",
    "\n",
    "_vtm_df = helper.vtm_calc(_fp_moment_df,\n",
    "                         baseline_time_1, baseline_time_2,\n",
    "                         tr_baseline_1, tr_baseline_2,\n",
    "                         fp_baseline_1, fp_baseline_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sync_offsets, _delta_time = helper.sync_offset_calc(_tr_corr_df_1, _tr_corr_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sync_offsets[:,0]-_sync_offsets[:,0], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 9 day Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename = 'hdf5/sanitized/9day.h5'\n",
    "\n",
    "print 'Blinding fixed probe runs'\n",
    "fp_moment_df_1 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_1')), blinds)\n",
    "fp_moment_df_2 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_2')), blinds)\n",
    "fp_moment_df_3 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_3')), blinds)\n",
    "fp_moment_df_4 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_4')), blinds)\n",
    "\n",
    "print 'Blinding trolley runs'\n",
    "tr_moment_df_1 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_1')), blinds)\n",
    "tr_moment_df_2 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_2')), blinds)\n",
    "tr_moment_df_3 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_3')), blinds)\n",
    "tr_moment_df_4 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_4')), blinds)\n",
    "tr_moment_df_5 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_5')), blinds)\n",
    "tr_moment_df_6 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_6')), blinds)\n",
    "\n",
    "print 'Correcting trolley footprint'\n",
    "tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "tr_corr_df_3 = helper_old.trolley_footprint_replacement(tr_moment_df_3)\n",
    "tr_corr_df_4 = helper_old.trolley_footprint_replacement(tr_moment_df_4)\n",
    "tr_corr_df_5 = helper_old.trolley_footprint_replacement(tr_moment_df_5)\n",
    "tr_corr_df_6 = helper_old.trolley_footprint_replacement(tr_moment_df_6)\n",
    "\n",
    "print 'Calculating baselines'\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "tr_baseline_3, fp_baseline_3, baseline_time_3, summed_azimuth_3, summed_pts_3 = helper_old.trolley_run_station_average(tr_corr_df_3)\n",
    "tr_baseline_4, fp_baseline_4, baseline_time_4, summed_azimuth_4, summed_pts_4 = helper_old.trolley_run_station_average(tr_corr_df_4)\n",
    "tr_baseline_5, fp_baseline_5, baseline_time_5, summed_azimuth_5, summed_pts_5 = helper_old.trolley_run_station_average(tr_corr_df_5)\n",
    "tr_baseline_6, fp_baseline_6, baseline_time_6, summed_azimuth_6, summed_pts_6 = helper_old.trolley_run_station_average(tr_corr_df_6)\n",
    "\n",
    "print 'Calculating virtual trolley measurements'\n",
    "vtm_df_1 = helper.vtm_calc(fp_moment_df_1,\n",
    "                           baseline_time_1, baseline_time_2,\n",
    "                           tr_baseline_1, tr_baseline_2,\n",
    "                           fp_baseline_1, fp_baseline_2)\n",
    "\n",
    "vtm_df_2 = helper.vtm_calc(fp_moment_df_2,\n",
    "                           baseline_time_3, baseline_time_4,\n",
    "                           tr_baseline_3, tr_baseline_4,\n",
    "                           fp_baseline_3, fp_baseline_4)\n",
    "\n",
    "vtm_df_3 = helper.vtm_calc(fp_moment_df_3,\n",
    "                           baseline_time_4, baseline_time_5,\n",
    "                           tr_baseline_4, tr_baseline_5,\n",
    "                           fp_baseline_4, fp_baseline_5)\n",
    "\n",
    "vtm_df_4 = helper.vtm_calc(fp_moment_df_4,\n",
    "                           baseline_time_5, baseline_time_6,\n",
    "                           tr_baseline_6, tr_baseline_6,\n",
    "                           fp_baseline_6, fp_baseline_6)\n",
    "\n",
    "print 'Saving vtm dataframes'\n",
    "filename = 'hdf5/9day_moments_blinded_sanitized.h5'\n",
    "vtm_df_1.to_hdf(filename, key='vtm_1')\n",
    "vtm_df_2.to_hdf(filename, key='vtm_2')\n",
    "vtm_df_3.to_hdf(filename, key='vtm_3')\n",
    "vtm_df_4.to_hdf(filename, key='vtm_4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print 'Blinding fixed probe runs'\n",
    "# fp_moment_df_1 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'fp_df_1')), blinds)\n",
    "# fp_moment_df_2 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'fp_df_2')), blinds)\n",
    "# fp_moment_df_3 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'fp_df_3')), blinds)\n",
    "# fp_moment_df_4 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'fp_df_4')), blinds)\n",
    "\n",
    "# print 'Blinding trolley runs'\n",
    "# tr_moment_df_1 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'tr_df_1')), blinds)\n",
    "# tr_moment_df_2 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'tr_df_2')), blinds)\n",
    "# tr_moment_df_3 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'tr_df_3')), blinds)\n",
    "# tr_moment_df_4 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'tr_df_4')), blinds)\n",
    "# tr_moment_df_5 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'tr_df_5')), blinds)\n",
    "# tr_moment_df_6 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/9day.h5', 'tr_df_6')), blinds)\n",
    "\n",
    "# print 'Correcting trolley footprint'\n",
    "# tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "# tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "# tr_corr_df_3 = helper_old.trolley_footprint_replacement(tr_moment_df_3)\n",
    "# tr_corr_df_4 = helper_old.trolley_footprint_replacement(tr_moment_df_4)\n",
    "# tr_corr_df_5 = helper_old.trolley_footprint_replacement(tr_moment_df_5)\n",
    "# tr_corr_df_6 = helper_old.trolley_footprint_replacement(tr_moment_df_6)\n",
    "\n",
    "#####################################\n",
    "# Implement \"Bloch-style\" by replacing st 1, 3, 5, and 54 with average of neighbors\n",
    "print 'Implementing Bloch-style treatment of stations 1, 3, 5, and 54'\n",
    "station_phi = trfp.STATION_BARCODE_PHI\n",
    "weight10 = (station_phi[2]-station_phi[1])/((station_phi[2]-station_phi[0]))\n",
    "weight12 = (station_phi[1]-station_phi[0])/((station_phi[2]-station_phi[0]))\n",
    "weight32 = (station_phi[4]-station_phi[3])/((station_phi[4]-(station_phi[2]-360)))\n",
    "weight34 = (station_phi[3]-(station_phi[2]-360))/((station_phi[4]-(station_phi[2]-360)))\n",
    "weight54 = (station_phi[6]-station_phi[5])/((station_phi[6]-station_phi[4]))\n",
    "weight56 = (station_phi[5]-station_phi[4])/((station_phi[6]-station_phi[4]))\n",
    "weight5453 = (station_phi[55]-station_phi[54])/((station_phi[55]-station_phi[53]))\n",
    "weight5455 = (station_phi[54]-station_phi[53])/((station_phi[55]-station_phi[53]))\n",
    "\n",
    "for dataframe in [fp_moment_df_1, fp_moment_df_2, fp_moment_df_3, fp_moment_df_4,\n",
    "                  tr_corr_df_1, tr_corr_df_2, tr_corr_df_3, tr_corr_df_4, tr_corr_df_5, tr_corr_df_6]:\n",
    "    for m in range(1,7):\n",
    "        print m\n",
    "        dataframe['st1,m'+str(m)] = weight10*dataframe['st0,m'+str(m)] + weight12*dataframe['st2,m'+str(m)]\n",
    "        dataframe['st3,m'+str(m)] = weight32*dataframe['st2,m'+str(m)] + weight34*dataframe['st4,m'+str(m)]\n",
    "        dataframe['st5,m'+str(m)] = weight54*dataframe['st4,m'+str(m)] + weight56*dataframe['st6,m'+str(m)]\n",
    "        dataframe['st54,m'+str(m)] = weight5453*dataframe['st53,m'+str(m)] + weight5455*dataframe['st55,m'+str(m)]\n",
    "\n",
    "#####################################\n",
    "\n",
    "print 'Calculating baselines'\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "tr_baseline_3, fp_baseline_3, baseline_time_3, summed_azimuth_3, summed_pts_3 = helper_old.trolley_run_station_average(tr_corr_df_3)\n",
    "tr_baseline_4, fp_baseline_4, baseline_time_4, summed_azimuth_4, summed_pts_4 = helper_old.trolley_run_station_average(tr_corr_df_4)\n",
    "tr_baseline_5, fp_baseline_5, baseline_time_5, summed_azimuth_5, summed_pts_5 = helper_old.trolley_run_station_average(tr_corr_df_5)\n",
    "tr_baseline_6, fp_baseline_6, baseline_time_6, summed_azimuth_6, summed_pts_6 = helper_old.trolley_run_station_average(tr_corr_df_6)\n",
    "\n",
    "print 'Calculating virtual trolley measurements'\n",
    "vtm_df_1 = helper.vtm_calc(fp_moment_df_1,\n",
    "                           baseline_time_1, baseline_time_2,\n",
    "                           tr_baseline_1, tr_baseline_2,\n",
    "                           fp_baseline_1, fp_baseline_2)\n",
    "\n",
    "vtm_df_2 = helper.vtm_calc(fp_moment_df_2,\n",
    "                           baseline_time_3, baseline_time_4,\n",
    "                           tr_baseline_3, tr_baseline_4,\n",
    "                           fp_baseline_3, fp_baseline_4)\n",
    "\n",
    "vtm_df_3 = helper.vtm_calc(fp_moment_df_3,\n",
    "                           baseline_time_4, baseline_time_5,\n",
    "                           tr_baseline_4, tr_baseline_5,\n",
    "                           fp_baseline_4, fp_baseline_5)\n",
    "\n",
    "vtm_df_4 = helper.vtm_calc(fp_moment_df_4,\n",
    "                           baseline_time_5, baseline_time_6,\n",
    "                           tr_baseline_6, tr_baseline_6,\n",
    "                           fp_baseline_6, fp_baseline_6)\n",
    "\n",
    "# print 'Saving vtm dataframes'\n",
    "# filename = 'hdf5/9day_moments_blinded_bloch-style.h5'\n",
    "# vtm_df_1.to_hdf(filename, key='vtm_1')\n",
    "# vtm_df_2.to_hdf(filename, key='vtm_2')\n",
    "# vtm_df_3.to_hdf(filename, key='vtm_3')\n",
    "# vtm_df_4.to_hdf(filename, key='vtm_4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Low Kick Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## High Kick Data Set\n",
    "\n",
    "print 'fp run 1'\n",
    "fp_moment_df_1 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf('hdf5/lowkick.h5', 'fp_df_1')), blinds)\n",
    "\n",
    "print 'tr run 1'\n",
    "tr_moment_df_1 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/lowkick.h5', 'tr_df_1')), blinds)\n",
    "print 'tr run 2'\n",
    "tr_moment_df_2 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf('hdf5/lowkick.h5', 'tr_df_2')), blinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "\n",
    "vtm_df_1 = helper.vtm_calc(fp_moment_df_1,\n",
    "                           baseline_time_1, baseline_time_2,\n",
    "                           tr_baseline_1, tr_baseline_2,\n",
    "                           fp_baseline_1, fp_baseline_2)\n",
    "\n",
    "start = np.ceil(vtm_df_1.index.values[0]/10)*10\n",
    "end = np.floor(vtm_df_1.index.values[-1]/10)*10 + 10\n",
    "bins_1 = np.arange(start, end, 10)  # bin edges\n",
    "bin_centers_1 = np.arange(start, end-10, 10)+5\n",
    "\n",
    "vtm_bin_df_1 = vtm_df_1.groupby(pd.cut(vtm_df_1.index, bins_1)).mean()\n",
    "vtm_bin_df_1.index = bin_centers_1\n",
    "\n",
    "vtm_bin_df_full = vtm_bin_df_1.copy()\n",
    "\n",
    "azi_avg_df = pd.DataFrame(np.zeros((vtm_bin_df_full.shape[0],6)),\n",
    "                         index = vtm_bin_df_full.index,\n",
    "                         columns = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "for m in range(5):\n",
    "    weight = summed_azimuth_1[:, m] + summed_azimuth_2[:, m]\n",
    "    total_weight = np.nansum(weight)\n",
    "    stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "    azi_avg_df['m'+str(m+1)] = vtm_bin_df_full[stm_list].multiply(weight).sum(axis=1)/total_weight\n",
    "    \n",
    "print_df = azi_avg_df[['m1','m2','m3','m5']].copy()/61.79\n",
    "print_df['m1_err'] = -1\n",
    "print_df['m2_err'] = -1\n",
    "print_df['m3_err'] = -1\n",
    "print_df['m5_err'] = -1\n",
    "\n",
    "print_df = print_df.sort_index(axis='columns')\n",
    "# print_df.head()\n",
    "# print_df.to_csv('purcell_60hr_7-31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(print_df.index.values, print_df['m1'], '.')\n",
    "plt.xlim((1526455000, 1526475000))\n",
    "plt.ylim(842,845)\n",
    "ax = plt.gca()\n",
    "fig = plt.gcf()\n",
    "\n",
    "plt2.plt_unix_time_to_CST(ax)\n",
    "\n",
    "fig.set_size_inches(12,6)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## look at naive trolley avg\n",
    "\n",
    "print np.dot(tr_baseline_1[:,0],summed_azimuth_1[:,0])/np.sum(summed_azimuth_1[:,0])/61.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## High Kick Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## High Kick Data Set\n",
    "\n",
    "filename = 'hdf5/highkick.h5'\n",
    "\n",
    "print 'fp run 1'\n",
    "fp_moment_df_1 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_1')), blinds)\n",
    "print 'fp run 2'\n",
    "fp_moment_df_2 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_2')), blinds)\n",
    "\n",
    "print 'tr run 1'\n",
    "tr_moment_df_1 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_1')), blinds)\n",
    "print 'tr run 2'\n",
    "tr_moment_df_2 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_2')), blinds)\n",
    "print 'tr run 3'\n",
    "tr_moment_df_3 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_3')), blinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "tr_corr_df_3 = helper_old.trolley_footprint_replacement(tr_moment_df_3)\n",
    "\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "tr_baseline_3, fp_baseline_3, baseline_time_3, summed_azimuth_3, summed_pts_3 = helper_old.trolley_run_station_average(tr_corr_df_3)\n",
    "\n",
    "vtm_df_1 = helper.vtm_calc(fp_moment_df_1,\n",
    "                           baseline_time_1, baseline_time_2,\n",
    "                           tr_baseline_1, tr_baseline_2,\n",
    "                           fp_baseline_1, fp_baseline_2)\n",
    "\n",
    "vtm_df_2 = helper.vtm_calc(fp_moment_df_2,\n",
    "                           baseline_time_2, baseline_time_3,\n",
    "                           tr_baseline_2, tr_baseline_3,\n",
    "                           fp_baseline_2, fp_baseline_3)\n",
    "\n",
    "print 'Saving vtm dataframes'\n",
    "filename = 'hdf5/highkick_moments_blinded.h5'\n",
    "vtm_df_1.to_hdf(filename, key='vtm_1')\n",
    "vtm_df_2.to_hdf(filename, key='vtm_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = np.ceil(vtm_df_1.index.values[0]/1000)*1000\n",
    "end = np.floor(vtm_df_1.index.values[-1]/1000)*1000 + 1000\n",
    "bins_1 = np.arange(start, end, 1000)  # bin edges\n",
    "bin_centers_1 = np.arange(start, end-1000, 1000)+500\n",
    "\n",
    "start = np.ceil(vtm_df_2.index.values[0]/1000)*1000\n",
    "end = np.floor(vtm_df_2.index.values[-1]/1000)*1000 + 1000\n",
    "bins_2 = np.arange(start, end, 1000)  # bin edges\n",
    "bin_centers_2 = np.arange(start, end-1000, 1000)+500\n",
    "\n",
    "vtm_bin_df_1 = vtm_df_1.groupby(pd.cut(vtm_df_1.index, bins_1)).mean()\n",
    "vtm_bin_df_1.index = bin_centers_1\n",
    "vtm_bin_df_2 = vtm_df_2.groupby(pd.cut(vtm_df_2.index, bins_2)).mean()\n",
    "vtm_bin_df_2.index = bin_centers_2\n",
    "\n",
    "vtm_bin_df_full = vtm_bin_df_1.copy()\n",
    "vtm_bin_df_full = vtm_bin_df_full.append(vtm_bin_df_2)\n",
    "\n",
    "azi_avg_df = pd.DataFrame(np.zeros((vtm_bin_df_full.shape[0],6)),\n",
    "                         index = vtm_bin_df_full.index,\n",
    "                         columns = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "for m in range(5):\n",
    "    weight = summed_azimuth_1[:, m] + summed_azimuth_2[:, m] + summed_azimuth_3[:, m]\n",
    "    total_weight = np.nansum(weight)\n",
    "    stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "    azi_avg_df['m'+str(m+1)] = vtm_bin_df_full[stm_list].multiply(weight).sum(axis=1)/total_weight\n",
    "    \n",
    "print_df = azi_avg_df[['m1','m2','m3','m5']].copy()/61.79\n",
    "print_df['m1_err'] = -1\n",
    "print_df['m2_err'] = -1\n",
    "print_df['m3_err'] = -1\n",
    "print_df['m5_err'] = -1\n",
    "\n",
    "print_df = print_df.sort_index(axis='columns')\n",
    "# print_df.head()\n",
    "# print_df.to_csv('purcell_60hr_7-31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print bins_1[0]\n",
    "print bins_1[-1]\n",
    "print bin_centers_1[0]\n",
    "print bin_centers_1[-1]\n",
    "\n",
    "print ''\n",
    "\n",
    "print bins_2[0]\n",
    "print bins_2[-1]\n",
    "print bin_centers_2[0]\n",
    "print bin_centers_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(print_df.index.values, print_df['m1'], '.')\n",
    "plt.xlim((1525117000, 1525120000))\n",
    "# plt.ylim(840.5,844)\n",
    "ax = plt.gca()\n",
    "fig = plt.gcf()\n",
    "\n",
    "fig.set_size_inches(12,6)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## End Game Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Endgame Data Set\n",
    "\n",
    "filename = 'hdf5/sanitized/endgame.h5'\n",
    "\n",
    "print 'fp run 1'\n",
    "# fp_moment_df_1 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_1')), blinds)\n",
    "print 'fp run 2'\n",
    "fp_moment_df_2 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_2')), blinds)\n",
    "print 'fp run 3'\n",
    "fp_moment_df_3 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_3')), blinds)\n",
    "print 'fp run 4'\n",
    "fp_moment_df_4 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_4')), blinds)\n",
    "print 'fp run 5'\n",
    "fp_moment_df_5 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_5')), blinds)\n",
    "print 'fp run 6'\n",
    "fp_moment_df_6 = apply_blinds_fp(helper.calc_moment_df(pd.read_hdf(filename, 'fp_df_6')), blinds)\n",
    "\n",
    "print 'tr run 1'\n",
    "# tr_moment_df_1 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_1')), blinds)\n",
    "print 'tr run 2'\n",
    "tr_moment_df_2 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_2')), blinds)\n",
    "print 'tr run 3'\n",
    "tr_moment_df_3 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_3')), blinds)\n",
    "print 'tr run 4'\n",
    "tr_moment_df_4 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_4')), blinds)\n",
    "print 'tr run 5'\n",
    "tr_moment_df_5 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_5')), blinds)\n",
    "print 'tr run 6'\n",
    "tr_moment_df_6 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_6')), blinds)\n",
    "print 'tr run 7'\n",
    "tr_moment_df_7 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_7')), blinds)\n",
    "print 'tr run 8'\n",
    "tr_moment_df_8 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_8')), blinds)\n",
    "print 'tr run 9'\n",
    "tr_moment_df_9 = apply_blinds_tr(helper.calc_moment_df(pd.read_hdf(filename, 'tr_df_9')), blinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tr_corr_df_1 = helper_old.trolley_footprint_replacement(tr_moment_df_1)\n",
    "tr_corr_df_2 = helper_old.trolley_footprint_replacement(tr_moment_df_2)\n",
    "tr_corr_df_3 = helper_old.trolley_footprint_replacement(tr_moment_df_3)\n",
    "tr_corr_df_4 = helper_old.trolley_footprint_replacement(tr_moment_df_4)\n",
    "tr_corr_df_5 = helper_old.trolley_footprint_replacement(tr_moment_df_5)\n",
    "tr_corr_df_6 = helper_old.trolley_footprint_replacement(tr_moment_df_6)\n",
    "tr_corr_df_7 = helper_old.trolley_footprint_replacement(tr_moment_df_7)\n",
    "tr_corr_df_8 = helper_old.trolley_footprint_replacement(tr_moment_df_8)\n",
    "# tr_corr_df_9 = helper_old.trolley_footprint_replacement(tr_moment_df_9)\n",
    "\n",
    "# tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, summed_pts_1 = helper_old.trolley_run_station_average(tr_corr_df_1)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, summed_pts_2 = helper_old.trolley_run_station_average(tr_corr_df_2)\n",
    "tr_baseline_3, fp_baseline_3, baseline_time_3, summed_azimuth_3, summed_pts_3 = helper_old.trolley_run_station_average(tr_corr_df_3)\n",
    "\n",
    "tr_baseline_4, fp_baseline_4, baseline_time_4, summed_azimuth_4, summed_pts_4 = helper_old.trolley_run_station_average(tr_corr_df_4)\n",
    "tr_baseline_5, fp_baseline_5, baseline_time_5, summed_azimuth_5, summed_pts_5 = helper_old.trolley_run_station_average(tr_corr_df_5)\n",
    "tr_baseline_6, fp_baseline_6, baseline_time_6, summed_azimuth_6, summed_pts_6 = helper_old.trolley_run_station_average(tr_corr_df_6)\n",
    "\n",
    "tr_baseline_7, fp_baseline_7, baseline_time_7, summed_azimuth_7, summed_pts_7 = helper_old.trolley_run_station_average(tr_corr_df_7)\n",
    "tr_baseline_8, fp_baseline_8, baseline_time_8, summed_azimuth_8, summed_pts_8 = helper_old.trolley_run_station_average(tr_corr_df_8)\n",
    "# tr_baseline_9, fp_baseline_9, baseline_time_9, summed_azimuth_9, summed_pts_9 = helper_old.trolley_run_station_average(tr_corr_df_9)\n",
    "\n",
    "## vtm_df_1 will have weirdness due to SCC shenanigans\n",
    "# vtm_df_1 = helper.vtm_calc(fp_moment_df_1,\n",
    "#                            baseline_time_1, baseline_time_2,  ## only backwards track?\n",
    "#                            tr_baseline_2, tr_baseline_2,\n",
    "#                            fp_baseline_2, fp_baseline_2)\n",
    "\n",
    "vtm_df_2 = helper.vtm_calc(fp_moment_df_2,\n",
    "                           baseline_time_2, baseline_time_3,\n",
    "                           tr_baseline_2, tr_baseline_3,\n",
    "                           fp_baseline_2, fp_baseline_3)\n",
    "\n",
    "vtm_df_3 = helper.vtm_calc(fp_moment_df_3,\n",
    "                           baseline_time_4, baseline_time_5,\n",
    "                           tr_baseline_4, tr_baseline_5,\n",
    "                           fp_baseline_4, fp_baseline_5)\n",
    "\n",
    "vtm_df_4 = helper.vtm_calc(fp_moment_df_4,\n",
    "                           baseline_time_6, baseline_time_7,\n",
    "                           tr_baseline_6, tr_baseline_7,\n",
    "                           fp_baseline_6, fp_baseline_7)\n",
    "\n",
    "vtm_df_5 = helper.vtm_calc(fp_moment_df_5,\n",
    "                           baseline_time_7, baseline_time_8,\n",
    "                           tr_baseline_7, tr_baseline_8,\n",
    "                           fp_baseline_7, fp_baseline_8)\n",
    "\n",
    "vtm_df_6 = helper.vtm_calc(fp_moment_df_6,\n",
    "                           baseline_time_8, baseline_time_7,  # NO BACKWARDS TRACKING\n",
    "                           tr_baseline_8, tr_baseline_8,\n",
    "                           fp_baseline_8, fp_baseline_8)\n",
    "\n",
    "print 'Saving vtm dataframes'\n",
    "filename = 'hdf5/endgame_moments_blinded_sanitized.h5'\n",
    "# vtm_df_1.to_hdf(filename, key='vtm_1')\n",
    "vtm_df_2.to_hdf(filename, key='vtm_2')\n",
    "vtm_df_3.to_hdf(filename, key='vtm_3')\n",
    "vtm_df_4.to_hdf(filename, key='vtm_4')\n",
    "vtm_df_5.to_hdf(filename, key='vtm_5')\n",
    "vtm_df_6.to_hdf(filename, key='vtm_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = np.ceil(vtm_df_1.index.values[0]/10)*10\n",
    "end = np.floor(vtm_df_1.index.values[-1]/10)*10 + 10\n",
    "bins_1 = np.arange(start, end, 10)  # bin edges\n",
    "bin_centers_1 = np.arange(start, end-10, 10)+5\n",
    "\n",
    "start = np.ceil(vtm_df_2.index.values[0]/10)*10\n",
    "end = np.floor(vtm_df_2.index.values[-1]/10)*10 + 10\n",
    "bins_2 = np.arange(start, end, 10)  # bin edges\n",
    "bin_centers_2 = np.arange(start, end-10, 10)+5\n",
    "\n",
    "start = np.ceil(vtm_df_3.index.values[0]/10)*10\n",
    "end = np.floor(vtm_df_3.index.values[-1]/10)*10 + 10\n",
    "bins_3 = np.arange(start, end, 10)  # bin edges\n",
    "bin_centers_3 = np.arange(start, end-10, 10)+5\n",
    "\n",
    "start = np.ceil(vtm_df_4.index.values[0]/10)*10\n",
    "end = np.floor(vtm_df_4.index.values[-1]/10)*10 + 10\n",
    "bins_4 = np.arange(start, end, 10)  # bin edges\n",
    "bin_centers_4 = np.arange(start, end-10, 10)+5\n",
    "\n",
    "start = np.ceil(vtm_df_5.index.values[0]/10)*10\n",
    "end = np.floor(vtm_df_5.index.values[-1]/10)*10 + 10\n",
    "bins_5 = np.arange(start, end, 10)  # bin edges\n",
    "bin_centers_5 = np.arange(start, end-10, 10)+5\n",
    "\n",
    "start = np.ceil(vtm_df_6.index.values[0]/10)*10\n",
    "end = np.floor(vtm_df_6.index.values[-1]/10)*10 + 10\n",
    "bins_6 = np.arange(start, end, 10)  # bin edges\n",
    "bin_centers_6 = np.arange(start, end-10, 10)+5\n",
    "\n",
    "vtm_bin_df_1 = vtm_df_1.groupby(pd.cut(vtm_df_1.index, bins_1)).mean()\n",
    "vtm_bin_df_1.index = bin_centers_1\n",
    "vtm_bin_df_2 = vtm_df_2.groupby(pd.cut(vtm_df_2.index, bins_2)).mean()\n",
    "vtm_bin_df_2.index = bin_centers_2\n",
    "vtm_bin_df_3 = vtm_df_3.groupby(pd.cut(vtm_df_3.index, bins_3)).mean()\n",
    "vtm_bin_df_3.index = bin_centers_3\n",
    "vtm_bin_df_4 = vtm_df_4.groupby(pd.cut(vtm_df_4.index, bins_4)).mean()\n",
    "vtm_bin_df_4.index = bin_centers_4\n",
    "vtm_bin_df_5 = vtm_df_5.groupby(pd.cut(vtm_df_5.index, bins_5)).mean()\n",
    "vtm_bin_df_5.index = bin_centers_5\n",
    "vtm_bin_df_6 = vtm_df_6.groupby(pd.cut(vtm_df_6.index, bins_6)).mean()\n",
    "vtm_bin_df_6.index = bin_centers_6\n",
    "\n",
    "vtm_bin_df_full = vtm_bin_df_1.copy()\n",
    "vtm_bin_df_full = vtm_bin_df_full.append(vtm_bin_df_2)\n",
    "vtm_bin_df_full = vtm_bin_df_full.append(vtm_bin_df_3)\n",
    "vtm_bin_df_full = vtm_bin_df_full.append(vtm_bin_df_4)\n",
    "vtm_bin_df_full = vtm_bin_df_full.append(vtm_bin_df_5)\n",
    "vtm_bin_df_full = vtm_bin_df_full.append(vtm_bin_df_6)\n",
    "\n",
    "azi_avg_df = pd.DataFrame(np.zeros((vtm_bin_df_full.shape[0],6)),\n",
    "                         index = vtm_bin_df_full.index,\n",
    "                         columns = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "for m in range(5):\n",
    "    weight = (summed_azimuth_1[:, m] + summed_azimuth_2[:, m] + summed_azimuth_3[:, m]\n",
    "             + summed_azimuth_4[:, m] + summed_azimuth_5[:, m] + summed_azimuth_6[:, m]\n",
    "             + summed_azimuth_7[:, m] + summed_azimuth_8[:, m])# + summed_azimuth_9[:, m])\n",
    "    total_weight = np.nansum(weight)\n",
    "    stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "    azi_avg_df['m'+str(m+1)] = vtm_bin_df_full[stm_list].multiply(weight).sum(axis=1)/total_weight\n",
    "    \n",
    "print_df = azi_avg_df[['m1','m2','m3','m5']].copy()/61.79\n",
    "print_df['m1_err'] = -1\n",
    "print_df['m2_err'] = -1\n",
    "print_df['m3_err'] = -1\n",
    "print_df['m5_err'] = -1\n",
    "\n",
    "print_df = print_df.sort_index(axis='columns')\n",
    "# print_df.head()\n",
    "# print_df.to_csv('purcell_60hr_7-31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(print_df.index.values, print_df['m1'], '.')\n",
    "# plt.xlim((1528650000, 1529000000))\n",
    "plt.ylim(843,845)\n",
    "ax = plt.gca()\n",
    "fig = plt.gcf()\n",
    "\n",
    "fig.set_size_inches(12,6)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sync_offsets, delta_t = helper.sync_offset_calc(tr_corr_df_5, tr_corr_df_6)\n",
    "\n",
    "def gaussian(x, A, x0, sigma): return A*np.exp(-(x-x0)**2/2./sigma**2)\n",
    "\n",
    "fig, axs = plt.subplots(2,3)\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        st = 3*i+j\n",
    "        if st == 5: continue\n",
    "            \n",
    "        plt.sca(axs[i,j])\n",
    "        hist, bins, _ = plt.hist(sync_offsets[:,st], bins=50)\n",
    "        low, high = axs[i,j].get_xlim()\n",
    "                \n",
    "        bins = bins[0:-1] + 0.5*(bins[1]-bins[0])\n",
    "        coeffs, _ = curve_fit(gaussian, bins, hist, p0=[1., 0., 10.])\n",
    "        fit = gaussian(np.arange(low, high, 0.1), coeffs[0], coeffs[1], coeffs[2])\n",
    "        plt.plot(np.arange(low,high,0.1), fit, label=r'$\\omega_0$ = '+str(np.round(coeffs[1],1))+'\\n$\\sigma$ = '+str(np.round(coeffs[2],1)))\n",
    "        plt.legend(loc=1)\n",
    "        plt2.plt_set_labels(axs[i,j], 'sync offset (Hz)', '', 'm '+str(st+1))\n",
    "        \n",
    "        if st == 0: plt.xlim(-100,100)\n",
    "        else: plt.xlim(-50,50)\n",
    "\n",
    "\n",
    "fig.set_size_inches(12,8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print np.mean(sync_offsets_1[:,0])/61.79\n",
    "print np.std(sync_offsets_1[:,0])/61.79\n",
    "print np.std(sync_offsets_1[:,0])/61.79/np.sqrt(72)\n",
    "\n",
    "plt.hist(sync_offsets_1[:,0]/61.7, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print np.mean(sync_offsets_1[:,0])/61.79\n",
    "print np.std(sync_offsets_1[:,0])/61.79\n",
    "print np.std(sync_offsets_1[:,0])/61.79/np.sqrt(72)\n",
    "\n",
    "plt.hist(sync_offsets_1[:,0]/61.7, bins=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
