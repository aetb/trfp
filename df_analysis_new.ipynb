{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import gm2\n",
    "import trfp\n",
    "import plotting_functions as plt2\n",
    "import analysis_helper as helper\n",
    "import helper_function_candidates as helper_old\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blinds = np.loadtxt('blinds.txt')\n",
    "\n",
    "def _apply_blinds_fp(input_df, blinds):\n",
    "    output_df = input_df.copy()\n",
    "    for m in range(6):\n",
    "        stms = ['st'+str(st)+',m'+str(m+1) for st in range(72)]\n",
    "        output_df[stms] = output_df[stms] + blinds[m]\n",
    "    return output_df\n",
    "\n",
    "def _apply_blinds_tr(input_df, blinds):\n",
    "    output_df = input_df.copy()\n",
    "    for m in range(6):\n",
    "        stms = ['st'+str(st)+',m'+str(m+1) for st in range(72)]\n",
    "        output_df[stms] = output_df[stms] + blinds[m]\n",
    "        trms = ['tr,m'+str(m+1)]\n",
    "        output_df[trms] = output_df[trms] + blinds[m]\n",
    "    return output_df\n",
    "\n",
    "def _get_keys(filename):\n",
    "    hdf = pd.HDFStore(filename, mode='r')\n",
    "    keys = hdf.keys()\n",
    "    hdf.close()\n",
    "    output_keys = []\n",
    "    for key in keys:\n",
    "        if key != '/subrun_df': output_keys.append(key[1:])\n",
    "    return output_keys\n",
    "\n",
    "def read_dfs(filename):\n",
    "    keys = _get_keys(filename)\n",
    "    interp_dfs = {}\n",
    "    for key in keys:\n",
    "        print '\\nReading ' + key\n",
    "        interp_dfs[key] = pd.read_hdf(filename, key=key)\n",
    "    subrun_df = pd.read_hdf(filename, key='subrun_df')\n",
    "    return interp_dfs, keys, subrun_df\n",
    "\n",
    "def interp_to_moment(interp_dfs, keys):\n",
    "    moment_dfs = {}\n",
    "    for key in keys:\n",
    "        print '\\nCalculating moments for ' + key\n",
    "        moment_dfs[key] = helper.calc_moment_df(interp_dfs[key])\n",
    "    return moment_dfs\n",
    "\n",
    "def blind_moments(moment_dfs, keys, blinds):\n",
    "    blinded_moment_dfs = {}\n",
    "    for key in keys:\n",
    "        print '\\nBlinding ' + key\n",
    "        if key[:2] == 'tr': blinded_moment_dfs[key] = _apply_blinds_tr(moment_dfs[key], blinds)\n",
    "        elif key[:2] == 'fp': blinded_moment_dfs[key] = _apply_blinds_fp(moment_dfs[key], blinds)\n",
    "        else: raise NameError('Unexpected key name.')\n",
    "    return blinded_moment_dfs\n",
    "            \n",
    "def moment_to_corrected(moment_dfs, keys):\n",
    "    corrected_dfs = {}\n",
    "    for key in keys:\n",
    "        if key[:2] == 'tr': \n",
    "            print \"\\nRemoving trolley footprints for \" + key\n",
    "            corrected_dfs[key] = helper_old.trolley_footprint_replacement(moment_dfs[key])\n",
    "        elif key[:2] == 'fp': corrected_dfs[key] = moment_dfs[key].copy()\n",
    "    print '\\n'\n",
    "    return corrected_dfs\n",
    "\n",
    "def bloch_style_moments(corrected_dfs, keys):\n",
    "    print '\\nImplementing Bloch-style treatment of stations 1, 3, 5, and 54'\n",
    "    station_phi = trfp.STATION_BARCODE_PHI\n",
    "    weight10 = (station_phi[2]-station_phi[1])/((station_phi[2]-station_phi[0]))\n",
    "    weight12 = (station_phi[1]-station_phi[0])/((station_phi[2]-station_phi[0]))\n",
    "    weight32 = (station_phi[4]-station_phi[3])/((station_phi[4]-(station_phi[2]-360)))\n",
    "    weight34 = (station_phi[3]-(station_phi[2]-360))/((station_phi[4]-(station_phi[2]-360)))\n",
    "    weight54 = (station_phi[6]-station_phi[5])/((station_phi[6]-station_phi[4]))\n",
    "    weight56 = (station_phi[5]-station_phi[4])/((station_phi[6]-station_phi[4]))\n",
    "    ## Nominally, Rachel updated her method to use station 54\n",
    "#     weight5453 = (station_phi[55]-station_phi[54])/((station_phi[55]-station_phi[53]))\n",
    "#     weight5455 = (station_phi[54]-station_phi[53])/((station_phi[55]-station_phi[53]))\n",
    "    \n",
    "    bloch_style_dfs = {}\n",
    "\n",
    "    for key in keys:\n",
    "        print key\n",
    "        bloch_style_dfs[key] = corrected_dfs[key].copy()\n",
    "        for m in range(1,7):\n",
    "            print 'm' +str(m)+'\\r',\n",
    "            bloch_style_dfs[key]['st1,m'+str(m)] = weight10*corrected_dfs[key]['st0,m'+str(m)]+ weight12*corrected_dfs[key]['st2,m'+str(m)]\n",
    "            bloch_style_dfs[key]['st3,m'+str(m)] = weight32*corrected_dfs[key]['st2,m'+str(m)] + weight34*corrected_dfs[key]['st4,m'+str(m)]\n",
    "            bloch_style_dfs[key]['st5,m'+str(m)] = weight54*corrected_dfs[key]['st4,m'+str(m)] + weight56*corrected_dfs[key]['st6,m'+str(m)]\n",
    "#             bloch_style_dfs[key]['st54,m'+str(m)] = weight5453*corrected_dfs[key]['st53,m'+str(m)] + weight5455*corrected_dfs[key]['st55,m'+str(m)]\n",
    "    \n",
    "    return bloch_style_dfs\n",
    "\n",
    "def station_average(corrected_dfs, keys):\n",
    "    '''\n",
    "    Ultimately, the baselines should be stored in a dataframe with the old array names as rows.\n",
    "    This will require rewriting `helper.vtm_calc` to use dfs instead of arrays.\n",
    "    This will help keep everything in the clean dictionary of dfs framework.\n",
    "    \n",
    "    Could also update `helper_old.trolley_run_station_average`.\n",
    "    '''\n",
    "    \n",
    "    print '\\nCalculating trolley run baselines.'\n",
    "    \n",
    "    tr_baselines = {}\n",
    "    fp_baselines = {}\n",
    "    baseline_times = {}\n",
    "    summed_azimuths = {}\n",
    "    summed_pts = {}\n",
    "    \n",
    "#     ## future-proofing\n",
    "#     station_avg_dfs = {}\n",
    "    \n",
    "    for key in keys:\n",
    "        if key[:2] == 'tr':\n",
    "            tr_baselines[key], fp_baselines[key], baseline_times[key], summed_azimuths[key], summed_pts[key] = helper_old.trolley_run_station_average(corrected_dfs[key])\n",
    "\n",
    "    baselines = {'tr':tr_baselines, 'fp':fp_baselines, 'time':baseline_times, 'azi':summed_azimuths, 'pts':summed_pts}\n",
    "            \n",
    "    return baselines\n",
    "\n",
    "## Defining a dictionary that, given a `fp` key, returns an array of the two bookending `tr` keys\n",
    "## for passing to the baselines dictionary.\n",
    "\n",
    "pair_dict_60hr = {'fp_df_1':['tr_df_1', 'tr_df_2']}\n",
    "pair_dict_9day = {'fp_df_1':['tr_df_1', 'tr_df_2'], 'fp_df_2':['tr_df_3', 'tr_df_4a'],\n",
    "                  'fp_df_3':['tr_df_4', 'tr_df_5'], 'fp_df_4':['tr_df_5', 'tr_df_6']}\n",
    "pair_dict_endgame = {'fp_df_2':['tr_df_2', 'tr_df_3'], 'fp_df_3':['tr_df_4', 'tr_df_5'],\n",
    "                     'fp_df_4':['tr_df_6', 'tr_df_7'], 'fp_df_5':['tr_df_7', 'tr_df_8'],\n",
    "                     'fp_df_6':['tr_df_8', 'tr_df_9']}\n",
    "pair_dict_highkick = {'fp_df_1':['tr_df_1', 'tr_df_2'], 'fp_df_2':['tr_df_2', 'tr_df_3']}\n",
    "\n",
    "\n",
    "def calculate_vtms(corrected_dfs, keys, baselines, pair_dict):\n",
    "    '''\n",
    "    Going to need some new dictionaries that define run pairs (and single-sided runs).\n",
    "    NOTE THIS NEEDS UPDATING FOR SINGLE SIDED RUNS.\n",
    "    Future update: Make this work on both fixed probe and trolley runs (might need to update `helper.vtm_calc`).\n",
    "    '''\n",
    "    \n",
    "    print '\\nCalculating VTMs'\n",
    "    \n",
    "    vtm_dfs = {}\n",
    "    \n",
    "    for key in keys:\n",
    "        if key[:2] == 'fp':\n",
    "            vtm_dfs[key] = helper.vtm_calc(corrected_dfs[key],\n",
    "                                           baselines['time'][pair_dict[key][0]], baselines['time'][pair_dict[key][1]],\n",
    "                                           baselines['tr'][pair_dict[key][0]], baselines['tr'][pair_dict[key][1]],\n",
    "                                           baselines['fp'][pair_dict[key][0]], baselines['fp'][pair_dict[key][1]]\n",
    "                                           )\n",
    "    \n",
    "    return vtm_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interp_dfs, keys, subrun_df = read_dfs('hdf5/sanitized/endgame.h5')\n",
    "pair_dict = pair_dict_endgame\n",
    "\n",
    "moment_dfs = interp_to_moment(interp_dfs, keys)\n",
    "# moment_dfs = blind_moments(moment_dfs, keys, blinds)\n",
    "corrected_dfs = moment_to_corrected(moment_dfs, keys)\n",
    "\n",
    "baselines = station_average(corrected_dfs, keys)\n",
    "\n",
    "## for endgame only\n",
    "for item in baselines:\n",
    "    baselines[item]['tr_df_9'] = baselines[item]['tr_df_8']\n",
    "baselines['time']['tr_df_9'] = baselines['time']['tr_df_7']\n",
    "\n",
    "vtm_dfs = calculate_vtms(corrected_dfs, keys, baselines, pair_dict)\n",
    "\n",
    "bloch_style_dfs = bloch_style_moments(corrected_dfs, keys)\n",
    "baselines_bloch = station_average(bloch_style_dfs, keys)\n",
    "\n",
    "## for endgame only\n",
    "for item in baselines_bloch:\n",
    "    baselines_bloch[item]['tr_df_9'] = baselines_bloch[item]['tr_df_8']\n",
    "baselines_bloch['time']['tr_df_9'] = baselines_bloch['time']['tr_df_7']\n",
    "\n",
    "vtm_dfs_bloch = calculate_vtms(bloch_style_dfs, keys, baselines_bloch, pair_dict)\n",
    "\n",
    "print '\\nComplete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vtm_dfs['fp_df_2']['st5,m1'], '.')\n",
    "plt.plot(vtm_dfs_bloch['fp_df_2']['st5,m1'], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vtm_dfs['fp_df_1'].to_hdf('hdf5/9day_vtms_unblinded_2020-04-15.h5', key='vtm_1')\n",
    "# vtm_dfs_bloch['fp_df_1'].to_hdf('hdf5/9day_blochlike_vtms_unblinded_2020-04-15.h5', key='vtm_1')\n",
    "\n",
    "vtm_dfs['fp_df_2'].to_hdf('hdf5/endgame_vtms_unblinded_2020-04-15.h5', key='vtm_2')\n",
    "vtm_dfs_bloch['fp_df_2'].to_hdf('hdf5/endgame_blochlike_vtms_unblinded_2020-04-15.h5', key='vtm_2')\n",
    "\n",
    "vtm_dfs['fp_df_3'].to_hdf('hdf5/endgame_vtms_unblinded_2020-04-15.h5', key='vtm_3')\n",
    "vtm_dfs_bloch['fp_df_3'].to_hdf('hdf5/endgame_blochlike_vtms_unblinded_2020-04-15.h5', key='vtm_3')\n",
    "\n",
    "vtm_dfs['fp_df_4'].to_hdf('hdf5/endgame_vtms_unblinded_2020-04-15.h5', key='vtm_4')\n",
    "vtm_dfs_bloch['fp_df_4'].to_hdf('hdf5/endgame_blochlike_vtms_unblinded_2020-04-15.h5', key='vtm_4')\n",
    "\n",
    "vtm_dfs['fp_df_5'].to_hdf('hdf5/endgame_vtms_unblinded_2020-04-15.h5', key='vtm_5')\n",
    "vtm_dfs_bloch['fp_df_5'].to_hdf('hdf5/endgame_blochlike_vtms_unblinded_2020-04-15.h5', key='vtm_5')\n",
    "\n",
    "vtm_dfs['fp_df_6'].to_hdf('hdf5/endgame_vtms_unblinded_2020-04-15.h5', key='vtm_6')\n",
    "vtm_dfs_bloch['fp_df_6'].to_hdf('hdf5/endgame_blochlike_vtms_unblinded_2020-04-15.h5', key='vtm_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_hdf('hdf5/highkick_blochlike_vtms_2020-03-05.h5', key='vtm_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
