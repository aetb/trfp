{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import trfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blinding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "\n",
    "def import_fp_moment_runs(file_path, all_runs, specific_runs=[]):\n",
    "    print 'Appending fixed probe runs.'\n",
    "    print 'Appending run ' + str(all_runs[0]) + '.',\n",
    "    single_runs = {}\n",
    "    fp_moment_df = pd.read_hdf(file_path, key='run_'+str(all_runs[0])+'_moment_df')\n",
    "    if all_runs[0] in specific_runs: single_runs[all_runs[0]] = fp_moment_df.copy()\n",
    "    for run in all_runs[1:]:\n",
    "        print '\\rAppending run ' + str(run) + '.',\n",
    "        temp_df = pd.read_hdf(file_path, key='run_'+str(run)+'_moment_df')\n",
    "        fp_moment_df = fp_moment_df.append(temp_df)\n",
    "        if run in specific_runs: single_runs[run] = temp_df.copy()\n",
    "    print '\\nDone appending fixed probe runs.'\n",
    "    if specific_runs: return fp_moment_df, single_runs\n",
    "    else: return fp_moment_df\n",
    "\n",
    "def import_tr_moment_runs(file_path, all_runs, specific_runs=[], corrected=True):\n",
    "    if corrected: print 'Appending corrected trolley runs.'\n",
    "    else: print 'Appending un-corrected trolley runs.'\n",
    "    print 'Appending run ' + str(all_runs[0]) + '.'\n",
    "    single_runs = {}\n",
    "    temp_df = pd.read_hdf(file_path, key='run_' + str(all_runs[0]) + '_moment_df')\n",
    "    if corrected:\n",
    "        if all_runs[0] in specific_runs: single_runs[all_runs[0]] = tr_output_df.copy()\n",
    "        tr_output_df = remove_trolley_effect(temp_df)\n",
    "    else:\n",
    "        if all_runs[0] in specific_runs: single_runs[all_runs[0]] = tr_output_df.copy()\n",
    "        tr_output_df = temp_df.copy()\n",
    "    for run in all_runs[1:]:\n",
    "        print 'Appending run ' + str(run) + '.'\n",
    "        temp_df = pd.read_hdf(file_path, key='run_' + str(run) + '_moment_df')\n",
    "        if corrected:\n",
    "            temp_df = remove_trolley_effect(temp_df)\n",
    "            if run in specific_runs: single_runs[run] = temp_df.copy()\n",
    "            tr_output_df = tr_output_df.append(temp_df)\n",
    "        else:\n",
    "            if run in specific_runs: single_runs[run] = temp_df.copy()\n",
    "            tr_output_df = tr_output_df.append(temp_df)\n",
    "    if corrected: print '\\nDone appending corrected trolley runs.'\n",
    "    else: print '\\nDone appending un-corrected trolley runs.'\n",
    "    if specific_runs: return tr_output_df, single_runs\n",
    "    else: return tr_output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[344.76  352.25  356.585   1.59    6.835  11.78   17.215  24.765  24.765\n",
      "  34.33   34.33   44.765  44.765  54.77   54.77   64.34   64.34   74.755\n",
      "  74.755  81.745  86.315  91.82   96.82  101.305 106.745 111.75  116.325\n",
      " 121.83  126.81  131.285 136.74  144.775 144.775 154.35  154.35  164.775\n",
      " 164.775 174.775 174.775 184.36  184.36  194.765 194.765 204.755 204.755\n",
      " 214.34  214.34  224.78  224.78  234.79  234.79  244.37  244.37  254.79\n",
      " 254.79  264.775 264.775 274.365 274.365 284.785 284.785 294.765 294.765\n",
      " 304.34  304.34  314.765 314.765 324.76  324.76  334.34  334.34  344.76\n",
      " 344.76 ]\n"
     ]
    }
   ],
   "source": [
    "# geometry things!\n",
    "### NEW!\n",
    "STATION_PROBE_NUMBER = [len(probes) for probes in trfp.STATION_PROBE_ID]\n",
    "\n",
    "STATION_BARCODE_PHI_6 = []\n",
    "for ii in range(72):\n",
    "    if STATION_PROBE_NUMBER[ii] == 6:\n",
    "        STATION_BARCODE_PHI_6.append(trfp.STATION_BARCODE_PHI[ii])\n",
    "        \n",
    "__EDGES_6 = (STATION_BARCODE_PHI_6+np.roll(STATION_BARCODE_PHI_6,1))/2\n",
    "if __EDGES_6[3] >= 180.:  # accounts for wrap around at station 3\n",
    "    __EDGES_6[3] = __EDGES_6[3]-180.\n",
    "else:\n",
    "    __EDGES_6[3] = __EDGES_6[3]+180.\n",
    "__EDGES_6 = np.append(__EDGES_6, __EDGES_6[0])\n",
    "\n",
    "STATION_BARCODE_EDGES_6 = []\n",
    "num_6_probe_stations = 0\n",
    "for ii in range(72):\n",
    "    if STATION_PROBE_NUMBER[ii] == 6:\n",
    "        STATION_BARCODE_EDGES_6.append(__EDGES_6[num_6_probe_stations])\n",
    "        num_6_probe_stations += 1\n",
    "    else:\n",
    "        STATION_BARCODE_EDGES_6.append(__EDGES_6[num_6_probe_stations])\n",
    "STATION_BARCODE_EDGES_6.append(STATION_BARCODE_EDGES_6[0])\n",
    "STATION_BARCODE_EDGES_6 = np.round(STATION_BARCODE_EDGES_6,3)\n",
    "\n",
    "print STATION_BARCODE_EDGES_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 7.49\n",
      "1: 4.335\n",
      "2: -354.995\n",
      "3: 5.245\n",
      "4: 4.945\n",
      "5: 5.435\n",
      "6: 7.55\n",
      "8: 9.565\n",
      "10: 10.435\n",
      "12: 10.005\n",
      "14: 9.57\n",
      "16: 10.415\n",
      "18: 6.99\n",
      "19: 4.57\n",
      "20: 5.505\n",
      "21: 5.0\n",
      "22: 4.485\n",
      "23: 5.44\n",
      "24: 5.005\n",
      "25: 4.575\n",
      "26: 5.505\n",
      "27: 4.98\n",
      "28: 4.475\n",
      "29: 5.455\n",
      "30: 8.035\n",
      "32: 9.575\n",
      "34: 10.425\n",
      "36: 10.0\n",
      "38: 9.585\n",
      "40: 10.405\n",
      "42: 9.99\n",
      "44: 9.585\n",
      "46: 10.44\n",
      "48: 10.01\n",
      "50: 9.58\n",
      "52: 10.42\n",
      "54: 9.985\n",
      "56: 9.59\n",
      "58: 10.42\n",
      "60: 9.98\n",
      "62: 9.575\n",
      "64: 10.425\n",
      "66: 9.995\n",
      "68: 9.58\n",
      "70: 10.42\n"
     ]
    }
   ],
   "source": [
    "for st in range(72):\n",
    "    if STATION_PROBE_NUMBER[st] == 6:\n",
    "        print str(st) + ': ' + str(np.round(np.diff(STATION_BARCODE_EDGES_6),3)[st])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the NEW trolley station averaging routine\n",
    "\n",
    "def trolley_run_station_average(corrected_df):\n",
    "\n",
    "    # tr_phi is not monotonic, so sort by tr_phi\n",
    "\n",
    "    corrected_df = corrected_df.sort_values(by=['tr_phi'])\n",
    "\n",
    "    measured_phi = corrected_df['tr_phi'].values\n",
    "    measured_extent = (np.roll(measured_phi,-1)-np.roll(measured_phi,1))/2\n",
    "    measured_extent[0] = measured_extent[0]+180\n",
    "    measured_extent[-1] = measured_extent[-1]+180\n",
    "    # print np.max(measured_extent)\n",
    "\n",
    "    corrected_df['tr_extent'] = pd.Series(measured_extent, index=corrected_df.index)\n",
    "    corrected_df = corrected_df.sort_index()\n",
    "\n",
    "    # for a given station:\n",
    "    # create a mask for when trolley is in [low edge, high edge)\n",
    "    tr_baseline = np.full((72,6), np.nan)\n",
    "    fp_baseline = np.full((72,6), np.nan)\n",
    "    summed_azimuth = np.full((72,6), np.nan)\n",
    "    summed_pts = np.full((72,6), np.nan)\n",
    "    baseline_time = np.full((72,6), np.nan)\n",
    "\n",
    "    for st in range(72):\n",
    "        \n",
    "        num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "        # first do m1-4 for all stations\n",
    "        \n",
    "        if station_edges[st+1] > station_edges[st]:\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges[st]) & (corrected_df['tr_phi'] < station_edges[st+1])\n",
    "        else:  # case where we go over the 360 deg line\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges[st]) | (corrected_df['tr_phi'] < station_edges[st+1])\n",
    "\n",
    "        out_df = corrected_df[mask].copy()\n",
    "        summed_pts[st, 0:4] = out_df.shape[0]\n",
    "        summed_azimuth[st, 0:4] = sum(out_df['tr_extent'].values)\n",
    "        baseline_time[st, 0:4] = sum(out_df.index.values)/summed_pts[st]\n",
    "\n",
    "        for m in range(4):\n",
    "            st_id = 'tr,m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                tr_baseline[st, m] = sum(out_df['tr_extent'].values*out_df[st_id].values)/sum(out_df['tr_extent'].values)\n",
    "\n",
    "            st_id = 'st'+str(st)+',m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                fp_baseline[st, m] = np.mean(out_df[st_id])\n",
    "        \n",
    "        if num_probes == 4: continue  # moves to next iteration for 4 probe stations\n",
    "\n",
    "        # next do m5+ for all 6-probe stations\n",
    "\n",
    "        if station_edges[st+1] > station_edges[st]:\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges_6_probe[st]) & (corrected_df['tr_phi'] < station_edges_6_probe[st+1])\n",
    "        else:  # case where we go over the 360 deg line\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges_6_probe[st]) | (corrected_df['tr_phi'] < station_edges_6_probe[st+1])\n",
    "        \n",
    "        out_df = corrected_df[mask].copy()\n",
    "        summed_pts[st, 4:6] = out_df.shape[0]\n",
    "        summed_azimuth_6[st, 4:6] = sum(out_df['tr_extent'].values)\n",
    "        baseline_time_6[st, 4:6] = sum(out_df.index.values)/summed_pts[st]\n",
    "\n",
    "        for m in range(4,6):\n",
    "            st_id = 'tr,m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                tr_baseline[st, m] = sum(out_df['tr_extent'].values*out_df[st_id].values)/sum(out_df['tr_extent'].values)\n",
    "\n",
    "            st_id = 'st'+str(st)+',m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                fp_baseline[st, m] = np.mean(out_df[st_id])\n",
    "    \n",
    "    return tr_baseline, fp_baseline, baseline_time, summed_azimuth, summed_pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate virutal trolley measurements\n",
    "\n",
    "def vtm_calc(fp_moment_df,\n",
    "             baseline_time_1, baseline_time_2,\n",
    "             tr_baseline_1, tr_baseline_2,\n",
    "             fp_baseline_1, fp_baseline_2):\n",
    "    \n",
    "    vtm_df = fp_moment_df.copy()\n",
    "    \n",
    "    for st in range(72):\n",
    "        num_probes = STATION_PROBE_NUMBER[st]\n",
    "        \n",
    "        if num_probes == 4:\n",
    "            num_moments = 4\n",
    "            if st == 41: J = trfp.J_4_PROBE_ST41\n",
    "            elif st == 37 | st == 39: J = trfp.J_4_PROBE_ST37_ST39\n",
    "            else: J = trfp.J_4_PROBE\n",
    "        else:\n",
    "            num_moments = 5\n",
    "            if st < 7: J = trfp.J_6_PROBE_OFFSET\n",
    "            else: J = trfp.J_6_PROBE\n",
    "        \n",
    "        # first subtract fixed probe baseline from vtm_df\n",
    "        for m in range(num_probes):\n",
    "            stm = 'st'+str(st)+',m'+str(m+1)\n",
    "\n",
    "            def __backwards_correction(time):\n",
    "                c1 = fp_baseline_1[st, m]\n",
    "                c2 = fp_baseline_2[st, m]\n",
    "                t1 = baseline_time_1[st, m]\n",
    "                t2 = baseline_time_2[st, m]\n",
    "                return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "\n",
    "            vtm_df[stm] = vtm_df[stm] - __backwards_correction(vtm_df.index.values)\n",
    "\n",
    "        # next apply the Jacobian to the station\n",
    "        for m in range(num_moments):\n",
    "            stm = 'st'+str(st)+',m'+str(m+1)\n",
    "            vtm_df[stm] = vtm_df[stm].dot(J[m])\n",
    "\n",
    "        # finally add trolley baseline to vtm_df\n",
    "        for m in range(num_probes):\n",
    "            stm = 'st'+str(st)+',m'+str(m+1)\n",
    "            \n",
    "            def __backwards_correction(time):\n",
    "                c1 = tr_baseline_1[st, m]\n",
    "                c2 = tr_baseline_2[st, m]\n",
    "                t1 = baseline_time_1[st, m]\n",
    "                t2 = baseline_time_2[st, m]\n",
    "                return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "\n",
    "            vtm_df[stm] = vtm_df[stm] + __backwards_correction(vtm_df.index.values)\n",
    "\n",
    "    return vtm_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trolley footprint removal\n",
    "\n",
    "def __split_by_nan(input_array):\n",
    "    return [input_array[clump] for clump in np.ma.clump_unmasked(np.ma.masked_invalid(input_array))]\n",
    "\n",
    "def trolley_footprint_replacement(moment_df, veto_low=False, veto_high=False):\n",
    "\n",
    "    nomask_df = moment_df.copy()\n",
    "    mask_df = nomask_df.copy()\n",
    "    temp_avg_df = pd.DataFrame(index=mask_df.index)\n",
    "\n",
    "    split_station = []\n",
    "    all_good_stations = np.arange(6,72)  # not using the inflector stations\n",
    "    no_ground_loop_stations = np.array(range(6,16)+range(64,72))  # valid for 25 deg veto\n",
    "    \n",
    "    if not veto_low:\n",
    "        barcode = trfp.STATION_BARCODE_PHI\n",
    "        veto_extent = 25\n",
    "        veto_low = (np.array(barcode)-(veto_extent-3)/2)%360\n",
    "        veto_high = (np.array(barcode)+3+(veto_extent-3)/2)%360\n",
    "\n",
    "    # first need to mask when trolley is near each station\n",
    "    for st in range(72):\n",
    "        stms = ['st' + str(st) + ',m' + str(m+1) for m in range(6)]\n",
    "\n",
    "        if veto_low[st] < veto_high[st]:\n",
    "            mask = (nomask_df['tr_phi']>veto_low[st]) & (nomask_df['tr_phi']<veto_high[st])\n",
    "        else:  # this happens when wrapping around 360 deg\n",
    "            mask = (nomask_df['tr_phi']>veto_low[st]) | (nomask_df['tr_phi']<veto_high[st])\n",
    "\n",
    "        if mask.iloc[0] & mask.iloc[-1]: split_station += [True]\n",
    "        else: split_station += [False]\n",
    "\n",
    "        mask_df[stms] = nomask_df[stms].mask(mask)\n",
    "\n",
    "        # next need to average all good stations that are not within 3 of current station\n",
    "        if st not in range(16, 23):  # note that these ranged were chosen for 25 deg veto\n",
    "            averaging_stations = np.delete(all_good_stations,\n",
    "                                           np.argwhere((np.abs((all_good_stations - st)%72)<=3)\n",
    "                                                       | (np.abs((all_good_stations - st)%72)>=69))\n",
    "                                          )\n",
    "        else:\n",
    "            averaging_stations = np.delete(no_ground_loop_stations,\n",
    "                                           np.argwhere((np.abs((no_ground_loop_stations - st)%72)<=3)\n",
    "                                                       | (np.abs((no_ground_loop_stations - st)%72)>=69))\n",
    "                                          )\n",
    "        for m in range(6):  # this will need to go over all moments\n",
    "            stm = 'st' + str(st) + ',m' + str(m+1)\n",
    "            avg_stms = ['st'+str(avg_st)+',m'+str(m+1) for avg_st in averaging_stations]\n",
    "            temp_avg_df[stm] = nomask_df[avg_stms].mean(axis=1).mask(~mask)\n",
    "\n",
    "    replaced_df = mask_df.copy()\n",
    "\n",
    "\n",
    "    # next need to remove the ring wide drift and replace with the station drift\n",
    "    for st in range(72):\n",
    "        num_moments = len(trfp.STATION_PROBE_ID[st])\n",
    "        for m in range(num_moments):\n",
    "            stm = 'st' + str(st) + ',m' + str(m+1)\n",
    "            num_endpts = 5\n",
    "            if not split_station[st]:\n",
    "\n",
    "                inner_splits = __split_by_nan(temp_avg_df[stm].values)\n",
    "                outer_splits = __split_by_nan(mask_df[stm].values)\n",
    "                first_inner_avg = np.mean(inner_splits[0][0:num_endpts])\n",
    "                last_inner_avg = np.mean(inner_splits[0][-num_endpts:])\n",
    "                first_outer_avg = np.mean(outer_splits[0][-num_endpts:])\n",
    "                last_outer_avg = np.mean(outer_splits[1][0:num_endpts])  # these all use 5 values to make extrapolation easier\n",
    "\n",
    "                inner_delta_y = 0.5 * (last_inner_avg-first_inner_avg)/inner_splits[0].size * (num_endpts-1)\n",
    "                outer_delta_y = 0.5 * (last_outer_avg-first_outer_avg)/inner_splits[0].size * (num_endpts-1)\n",
    "                inner_lin_fit = np.linspace(first_inner_avg-inner_delta_y, last_inner_avg+inner_delta_y, num=inner_splits[0].size)\n",
    "                outer_lin_fit = np.linspace(first_outer_avg+outer_delta_y, last_outer_avg-outer_delta_y, num=inner_splits[0].size)\n",
    "\n",
    "                replacement_values = inner_splits[0] - inner_lin_fit + outer_lin_fit\n",
    "                replaced_df[stm][replaced_df[stm].isna()] = np.array(replacement_values)\n",
    "\n",
    "            else:\n",
    "\n",
    "                inner_splits = __split_by_nan(temp_avg_df[stm].values)\n",
    "                first_inner_avg = [np.mean(inner_splits[0][0:num_endpts]), np.mean(inner_splits[1][0:num_endpts])]\n",
    "                last_inner_avg = [np.mean(inner_splits[0][-num_endpts:]), np.mean(inner_splits[1][-num_endpts:])]\n",
    "                inner_delta_y = [0.5 * (last_inner_avg[0]-first_inner_avg[0])/inner_splits[0].size * (num_endpts-1),\n",
    "                                 0.5 * (last_inner_avg[1]-first_inner_avg[1])/inner_splits[1].size * (num_endpts-1)]\n",
    "\n",
    "                first_inner_fit = np.linspace(first_inner_avg[0]-inner_delta_y[0],\n",
    "                                              last_inner_avg[0]+inner_delta_y[0],\n",
    "                                              num=inner_splits[0].size)\n",
    "                second_inner_fit = np.linspace(first_inner_avg[1]-inner_delta_y[1],\n",
    "                                              last_inner_avg[1]+inner_delta_y[1],\n",
    "                                              num=inner_splits[1].size)\n",
    "\n",
    "                # use 260 seconds of data after (or before) the vetoed window to make a linear fit to approximate \"station drift\"\n",
    "                outer_splits = __split_by_nan(mask_df[stm].values)\n",
    "                dt = 1  # the time step, usually 1 sec, but might as well make it a variable\n",
    "                num_pts = 260//dt  # integer period of 130 sec signal\n",
    "                first_outer_fit_coeffs = np.polyfit(np.arange(num_pts)*dt, outer_splits[0][0:num_pts], deg=1)\n",
    "                second_outer_fit_coeffs = np.polyfit(np.arange(num_pts)*dt, outer_splits[0][-num_pts:], deg=1)\n",
    "                first_outer_fit = np.polyval(first_outer_fit_coeffs,\n",
    "                                             np.linspace(inner_splits[0].size*-dt, -dt, num=inner_splits[0].size)\n",
    "                                            )\n",
    "                second_outer_fit = np.polyval(second_outer_fit_coeffs,\n",
    "                                              np.linspace(num_pts*dt, (num_pts+inner_splits[0].size)*dt,\n",
    "                                                          num=inner_splits[1].size)\n",
    "                                             )\n",
    "\n",
    "                first_replacement_values = inner_splits[0] - first_inner_fit + first_outer_fit\n",
    "                second_replacement_values = inner_splits[1] - second_inner_fit + second_outer_fit\n",
    "                replacement_values = np.append(first_replacement_values, second_replacement_values)\n",
    "                replaced_df[stm][replaced_df[stm].isna()] = np.array(replacement_values)\n",
    "\n",
    "    return replaced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trolley_moment_df = pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3956_moment_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = trolley_footprint_replacement(trolley_moment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
