{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/04\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import gm2\n",
    "import trfp\n",
    "\n",
    "STATION_BARCODE_PHI = [350.17, 354.33, 358.84, 4.34, 9.33,\n",
    "                       14.23, 20.2, 23.32, 29.33, 34.33, 39.33,\n",
    "                       43.31, 50.2, 53.31, 59.34, 64.33, 69.34,\n",
    "                       73.3, 80.17, 83.32, 89.31, 94.33, 99.31,\n",
    "                       103.3, 110.19, 113.31, 119.34, 124.32,\n",
    "                       129.3, 133.27, 140.21, 143.38, 149.34,\n",
    "                       154.33, 159.36, 163.31, 170.19, 173.39,\n",
    "                       179.36, 184.35, 189.36, 193.31, 200.17,\n",
    "                       203.38, 209.34, 214.31, 219.34, 223.32,\n",
    "                       230.22, 233.42, 239.36, 244.34, 249.38,\n",
    "                       253.34, 260.2, 263.47, 269.35, 274.34,\n",
    "                       279.38, 283.41, 290.19, 293.37, 299.34,\n",
    "                       304.31, 309.34, 313.29, 320.19, 323.41,\n",
    "                       329.33, 334.35, 339.35, 343.41]\n",
    "\n",
    "STATION_BARCODE_EDGES = (STATION_BARCODE_PHI+np.roll(STATION_BARCODE_PHI,1))/2\n",
    "if STATION_BARCODE_EDGES[3] >= 180.:  # accounts for wrap around at station 3\n",
    "    STATION_BARCODE_EDGES[3] = STATION_BARCODE_EDGES[3]-180.\n",
    "else:\n",
    "    STATION_BARCODE_EDGES[3] = STATION_BARCODE_EDGES[3]+180.\n",
    "STATION_BARCODE_EDGES = np.append(STATION_BARCODE_EDGES, STATION_BARCODE_EDGES[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished removing trolley images from 4382 events.                                                            \n",
      "Finished removing trolley images from 4359 events.                                                            \n",
      "Appending fixed probe runs.\n",
      "Appending run 3994.               \n",
      "Appending trolley runs.\n",
      "Finished removing trolley images from 893 events.                          \n",
      "Finished removing trolley images from 4382 events.                                                            \n",
      "Finished removing trolley images from 2977 events.                                                            \n",
      "Finished removing trolley images from 539 events.              \n",
      "Finished removing trolley images from 365 events.             \n",
      "Finished removing trolley images from 921 events.                         \n",
      "Finished removing trolley images from 4359 events.                                                            \n",
      "Finished removing trolley images from 2853 events.                                                            \n",
      "Appending run 3998. \n",
      "Done appending runs.\n",
      "\n",
      "Subtracting fixed probe baselines.\n",
      "stm: st71,m4.    \n",
      "\n",
      "Applying Jacobian.\n",
      "stm: st71,m4.          \n",
      "\n",
      "Adding trolley baselines.\n",
      "stm: st71,m4.    \n",
      "\n",
      "Done.\n",
      "[[2.50000000e+01 5.19582427e+04 1.07208897e+01 2.25000528e+01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "vetos = [25]\n",
    "output = np.zeros((7,4))\n",
    "ii = 0\n",
    "for veto_extent in vetos:\n",
    "\n",
    "    def remove_trolley_effect(trolley_moment_df):\n",
    "        '''DOC STRING'''\n",
    "        barcode = STATION_BARCODE_PHI\n",
    "\n",
    "        trolley_effect_removed_df = trolley_moment_df.copy()\n",
    "\n",
    "        for st in range(72):\n",
    "            print '\\rRemoving trolley image from station ' + str(st) + '.',\n",
    "            for m in range(1,7):\n",
    "                st_m = 'st' + str(st) + \",m\" + str(m)\n",
    "\n",
    "                # Unwrap the fixed probe data versus trolley position\n",
    "                raw_data = trolley_moment_df[['tr_phi', st_m]].copy()\n",
    "                raw_low = raw_data.copy()\n",
    "                raw_high = raw_data.copy()\n",
    "                raw_low['tr_phi'] = raw_low['tr_phi'] - 360\n",
    "                raw_high['tr_phi'] = raw_high['tr_phi'] + 360\n",
    "                unwrap_nomask_df = pd.concat([raw_low, raw_data, raw_high])\n",
    "\n",
    "                unwrap_mask_df = unwrap_nomask_df.copy()\n",
    "    #             # change mask to exclude 15 degrees instead of 7 (4 more degrees in either direction)\n",
    "    #             mask = ((unwrap_nomask_df['tr_phi']>barcode[st]-6) & (unwrap_nomask_df['tr_phi']<barcode[st]+9) |\n",
    "    #                     (unwrap_nomask_df['tr_phi']>barcode[st]-6-360) & (unwrap_nomask_df['tr_phi']<barcode[st]+9-360) |\n",
    "    #                     (unwrap_nomask_df['tr_phi']>barcode[st]-6+360) & (unwrap_nomask_df['tr_phi']<barcode[st]+9+360))\n",
    "                # change mask to exclude 22 degrees\n",
    "                veto_adjust = (veto_extent-7)/2\n",
    "                mask = ((unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust) |\n",
    "                        (unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust-360) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust-360) |\n",
    "                        (unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust+360) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust+360))\n",
    "                unwrap_mask_df[st_m] = unwrap_nomask_df[st_m].mask(mask)\n",
    "                unwrap_mask_df['tr_phi'] = unwrap_nomask_df['tr_phi']\n",
    "\n",
    "                unwrap_filled_df = unwrap_mask_df.copy()\n",
    "                temp = unwrap_filled_df.rolling(int(500),win_type='triang',min_periods=1,center=True).mean()\n",
    "                temp = temp.rolling(int(500),win_type='triang',min_periods=1,center=True).mean()\n",
    "                unwrap_filled_df[st_m] = unwrap_filled_df[st_m].mask(mask, temp[st_m])\n",
    "\n",
    "                length = raw_data.shape[0]\n",
    "                filled_df = unwrap_filled_df.iloc[length:2*length,:]\n",
    "\n",
    "                trolley_effect_removed_df[st_m] = filled_df[st_m]\n",
    "\n",
    "        print '\\rFinished removing trolley images from ' + str(length) + ' events.'\n",
    "        return trolley_effect_removed_df\n",
    "\n",
    "    station_phi = STATION_BARCODE_PHI\n",
    "    station_edges = STATION_BARCODE_EDGES\n",
    "\n",
    "    corrected_df_1 = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3956_moment_df'))\n",
    "    tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, _ = trfp.trolley_run_station_average(corrected_df_1)\n",
    "\n",
    "    corrected_df_2 = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_2.h5', key='run_3997_moment_df'))\n",
    "    tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, _ = trfp.trolley_run_station_average(corrected_df_2)\n",
    "\n",
    "    # load all fixed probe runs moment_df into one big moment_df (runs 3959--3994)\n",
    "    print 'Appending fixed probe runs.'\n",
    "    fp_moment_df = pd.read_hdf('60hr_fixed_probe_runs.h5', key='run_3959_moment_df')\n",
    "    pts = fp_moment_df.shape[0]\n",
    "    for run in np.arange(3959, 3995):\n",
    "        temp_df = pd.read_hdf('60hr_fixed_probe_runs.h5', key='run_'+str(run)+'_moment_df')\n",
    "        pts = pts + temp_df.shape[0]\n",
    "        print '\\rAppending run ' + str(run) + '.',\n",
    "        fp_moment_df = fp_moment_df.append(temp_df)\n",
    "\n",
    "    # load all the trolley runs corrected_df into on big file (to show window of 60hr set) ()\n",
    "    print '\\nAppending trolley runs.'\n",
    "    tr_corrected_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3955_moment_df'))\n",
    "    for run in np.arange(3956, 3959):\n",
    "        temp_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_'+str(run)+'_moment_df'))\n",
    "        pts = pts + temp_df.shape[0]\n",
    "        print '\\rAppending run ' + str(run) + '.',\n",
    "        tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "    for run in np.arange(3995, 3999):\n",
    "        temp_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_2.h5', key='run_'+str(run)+'_moment_df'))\n",
    "        pts = pts + temp_df.shape[0]\n",
    "        print '\\rAppending run ' + str(run) + '.',\n",
    "        tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "\n",
    "    print '\\nDone appending runs.'\n",
    "\n",
    "    ### Apply baseline corrections, generate virtual trolley measurements\n",
    "\n",
    "    fp_moment_baseline = fp_moment_df.copy()\n",
    "    fp_moment_tr_run_baseline = tr_corrected_df.copy()\n",
    "\n",
    "    # apply baseline corrections to each fp stm\n",
    "    print \"\\nSubtracting fixed probe baselines.\"\n",
    "    for st in np.arange(72):\n",
    "        num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "        for m in np.arange(num_probes):\n",
    "            stm = 'st'+str(st)+',m'+str(m+1)\n",
    "\n",
    "            def backwards_correction(time):\n",
    "                c1 = fp_baseline_1[st, m]\n",
    "                c2 = fp_baseline_2[st, m]\n",
    "                t1 = baseline_time_1[st]\n",
    "                t2 = baseline_time_2[st]\n",
    "                return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "\n",
    "            correction = backwards_correction(fp_moment_baseline.index.values)\n",
    "            fp_moment_baseline[stm] = fp_moment_baseline[stm] - correction\n",
    "            correction = backwards_correction(fp_moment_tr_run_baseline.index.values)\n",
    "            fp_moment_tr_run_baseline[stm] = fp_moment_tr_run_baseline[stm] - correction\n",
    "\n",
    "            print '\\rstm: ' + stm + '.',\n",
    "\n",
    "    # replace columns in vtr with Jacobian-fixed columns from fp baseline correction\n",
    "\n",
    "    print \"\\n\\nApplying Jacobian.\"\n",
    "    vtr_df = fp_moment_df.copy()\n",
    "    vtr_tr_run_df = tr_corrected_df.copy()\n",
    "\n",
    "    for st in np.arange(72):\n",
    "        num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "        if num_probes == 4:\n",
    "            num_moments = 4\n",
    "            if st == 41:\n",
    "                J = trfp.J_4_PROBE_ST41\n",
    "            elif st == 37 | st == 39:\n",
    "                J = trfp.J_4_PROBE_ST37_ST39\n",
    "            else:\n",
    "                J = trfp.J_4_PROBE\n",
    "        else:\n",
    "            num_moments = 5\n",
    "            if st < 7:\n",
    "                J = trfp.J_6_PROBE_OFFSET\n",
    "            else:\n",
    "                J = trfp.J_6_PROBE\n",
    "        # run over each vtr moment:\n",
    "        for m in np.arange(num_moments):\n",
    "            vtr_stm = 'st'+str(st)+',m'+str(m+1)\n",
    "            fp_stm = ['st'+str(st)+',m'+str(fp_m+1) for fp_m in np.arange(num_moments)]\n",
    "            vtr_df[vtr_stm] = fp_moment_baseline[fp_stm].dot(J[m])\n",
    "            vtr_tr_run_df[vtr_stm] = fp_moment_tr_run_baseline[fp_stm].dot(J[m])\n",
    "\n",
    "            print '\\rstm: ' + stm + '.',\n",
    "\n",
    "    # Add trolley baseline correction (with backwards correction)\n",
    "    print \"\\n\\nAdding trolley baselines.\"\n",
    "\n",
    "    for st in np.arange(72):\n",
    "        num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "        for m in np.arange(num_probes):\n",
    "            stm = 'st'+str(st)+',m'+str(m+1)\n",
    "\n",
    "            def backwards_correction(time):\n",
    "                c1 = tr_baseline_1[st, m]\n",
    "                c2 = tr_baseline_2[st, m]\n",
    "                t1 = baseline_time_1[st]\n",
    "                t2 = baseline_time_2[st]\n",
    "                return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "\n",
    "            correction = backwards_correction(vtr_df.index.values)\n",
    "            vtr_df[stm] = vtr_df[stm] + correction\n",
    "            correction = backwards_correction(vtr_tr_run_df.index.values)\n",
    "            vtr_tr_run_df[stm] = vtr_tr_run_df[stm] + correction\n",
    "\n",
    "            print '\\rstm: ' + stm + '.',\n",
    "\n",
    "    print '\\n\\nDone.'\n",
    "\n",
    "    # bin into the agreed upon bins\n",
    "\n",
    "    bins = np.arange(1524384055, 1524641055, 1000)-500\n",
    "    bin_centers = np.arange(1524384055, 1524640055, 1000)\n",
    "\n",
    "    # bins = np.arange(1524383560, 1524640850, 10)-5\n",
    "    # bin_centers = np.arange(1524383560, 1524640840, 10)\n",
    "\n",
    "    vtr_time_bin_df = vtr_df.groupby(pd.cut(vtr_df.index,bins)).mean()\n",
    "    vtr_time_bin_df.index = bin_centers\n",
    "    vtr_time_bin_df.head()\n",
    "\n",
    "\n",
    "    # print vtr_time_bin_df.iloc[[0,-1]]\n",
    "    # print vtr_time_bin_df.shape\n",
    "    # print bin_centers.shape\n",
    "\n",
    "    test_df = vtr_time_bin_df.copy()\n",
    "\n",
    "    azi_avg_df = pd.DataFrame(np.zeros((test_df.shape[0],6)),\n",
    "                             index = test_df.index,\n",
    "                             columns = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "    weight = summed_azimuth_1+summed_azimuth_2\n",
    "    total_weight = np.sum(weight)\n",
    "\n",
    "    for m in np.arange(6):\n",
    "        stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "        azi_avg_df['m'+str(m+1)] = test_df[stm_list].multiply(weight).sum(axis=1)/total_weight\n",
    "        \n",
    "    output[ii, 0] = veto_extent\n",
    "    output[ii, 1] = azi_avg_df.loc[1524639055]['m1']\n",
    "    output[ii, 2] = azi_avg_df.loc[1524639055]['m2']\n",
    "    output[ii, 3] = azi_avg_df.loc[1524639055]['m3']\n",
    "    \n",
    "    print output\n",
    "    ii += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = azi_avg_df[['m1', 'm2', 'm3', 'm5']].copy()\n",
    "\n",
    "review_df['m1_unc'] = 0.116\n",
    "review_df['m2_unc'] = 0.060\n",
    "review_df['m3_unc'] = 0.062\n",
    "review_df['m5_unc'] = 0.100\n",
    "\n",
    "review_df['m1'] = review_df['m1']/61.79\n",
    "review_df['m2'] = review_df['m2']/61.79\n",
    "review_df['m3'] = review_df['m3']/61.79\n",
    "review_df['m5'] = review_df['m5']/61.79\n",
    "\n",
    "review_df = review_df.reindex(sorted(review_df.columns), axis=1)\n",
    "\n",
    "review_df.head()\n",
    "\n",
    "review_df.to_csv('purcell_may22_25degveto.csv', float_format='%0.4f', index_label='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azi_avg_df.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATION_BARCODE_PHI = [350.17, 354.33, 358.84, 4.34, 9.33,\n",
    "                       14.23, 20.2, 23.32, 29.33, 34.33, 39.33,\n",
    "                       43.31, 50.2, 53.31, 59.34, 64.33, 69.34,\n",
    "                       73.3, 80.17, 83.32, 89.31, 94.33, 99.31,\n",
    "                       103.3, 110.19, 113.31, 119.34, 124.32,\n",
    "                       129.3, 133.27, 140.21, 143.38, 149.34,\n",
    "                       154.33, 159.36, 163.31, 170.19, 173.39,\n",
    "                       179.36, 184.35, 189.36, 193.31, 200.17,\n",
    "                       203.38, 209.34, 214.31, 219.34, 223.32,\n",
    "                       230.22, 233.42, 239.36, 244.34, 249.38,\n",
    "                       253.34, 260.2, 263.47, 269.35, 274.34,\n",
    "                       279.38, 283.41, 290.19, 293.37, 299.34,\n",
    "                       304.31, 309.34, 313.29, 320.19, 323.41,\n",
    "                       329.33, 334.35, 339.35, 343.41]\n",
    "\n",
    "STATION_BARCODE_EDGES = (STATION_BARCODE_PHI+np.roll(STATION_BARCODE_PHI,1))/2\n",
    "if STATION_BARCODE_EDGES[3] >= 180.:  # accounts for wrap around at station 3\n",
    "    STATION_BARCODE_EDGES[3] = STATION_BARCODE_EDGES[3]-180.\n",
    "else:\n",
    "    STATION_BARCODE_EDGES[3] = STATION_BARCODE_EDGES[3]+180.\n",
    "STATION_BARCODE_EDGES = np.append(STATION_BARCODE_EDGES, STATION_BARCODE_EDGES[0])\n",
    "\n",
    "STATION_BARCODE_EDGES[1] = STATION_BARCODE_PHI[1]\n",
    "STATION_BARCODE_EDGES[2] = STATION_BARCODE_PHI[1]\n",
    "STATION_BARCODE_EDGES[3] = STATION_BARCODE_PHI[3]\n",
    "STATION_BARCODE_EDGES[4] = STATION_BARCODE_PHI[3]\n",
    "STATION_BARCODE_EDGES[5] = STATION_BARCODE_PHI[5]\n",
    "STATION_BARCODE_EDGES[6] = STATION_BARCODE_PHI[5]\n",
    "STATION_BARCODE_EDGES[54] = STATION_BARCODE_PHI[54]\n",
    "STATION_BARCODE_EDGES[55] = STATION_BARCODE_PHI[54]\n",
    "\n",
    "print STATION_BARCODE_EDGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trolley_run_station_average(corrected_df):\n",
    "    station_phi = STATION_BARCODE_PHI\n",
    "    station_edges = STATION_BARCODE_EDGES\n",
    "\n",
    "    # tr_phi is not monotonic, so sort by tr_phi\n",
    "\n",
    "    corrected_df = corrected_df.sort_values(by=['tr_phi'])\n",
    "\n",
    "    measured_phi = corrected_df['tr_phi'].values\n",
    "    measured_extent = (np.roll(measured_phi,-1)-np.roll(measured_phi,1))/2\n",
    "    measured_extent[0] = measured_extent[0]+180\n",
    "    measured_extent[-1] = measured_extent[-1]+180\n",
    "    # print np.max(measured_extent)\n",
    "\n",
    "    corrected_df['tr_extent'] = pd.Series(measured_extent, index=corrected_df.index)\n",
    "    corrected_df = corrected_df.sort_index()\n",
    "\n",
    "    # for a given station:\n",
    "    # create a mask for when trolley is in [low edge, high edge)\n",
    "    tr_baseline = np.empty([72,17])\n",
    "    fp_baseline = np.empty([72,6])\n",
    "    summed_azimuth = np.empty(72)\n",
    "    summed_pts = np.empty(72)\n",
    "    baseline_time = np.empty(72)\n",
    "    tr_baseline[:] = np.nan\n",
    "    fp_baseline[:] = np.nan\n",
    "    summed_azimuth[:] = np.nan\n",
    "    summed_pts[:] = np.nan\n",
    "    baseline_time[:] = np.nan\n",
    "\n",
    "    for st in range(72): \n",
    "        if station_edges[st+1] > station_edges[st]:\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges[st]) & (corrected_df['tr_phi'] < station_edges[st+1])\n",
    "        else:  # case where we go over the 360 deg line\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges[st]) | (corrected_df['tr_phi'] < station_edges[st+1])\n",
    "\n",
    "        out_df = corrected_df[mask]\n",
    "        summed_pts[st] = out_df.shape[0]\n",
    "        summed_azimuth[st] = sum(out_df['tr_extent'].values)        \n",
    "        baseline_time[st] = sum(out_df.index.values)/summed_pts[st]\n",
    "\n",
    "        for m in range(17):\n",
    "            st_id = 'tr,m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                tr_baseline[st, m] = sum(out_df['tr_extent'].values*out_df[st_id].values)/sum(out_df['tr_extent'].values)\n",
    "            else:\n",
    "                tr_baseline[st, m] = np.nan\n",
    "        for m in range(6):\n",
    "            st_id = 'st'+str(st)+',m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                fp_baseline[st, m] = np.mean(out_df[st_id])\n",
    "            else:\n",
    "                fp_baseline[st, m] = np.nan\n",
    "    \n",
    "    return tr_baseline, fp_baseline, baseline_time, summed_azimuth, summed_pts\n",
    "\n",
    "def remove_trolley_effect(trolley_moment_df):\n",
    "    '''DOC STRING'''\n",
    "    barcode = STATION_BARCODE_PHI\n",
    "\n",
    "    trolley_effect_removed_df = trolley_moment_df.copy()\n",
    "\n",
    "    for st in range(72):\n",
    "        print '\\rRemoving trolley image from station ' + str(st) + '.',\n",
    "        for m in range(1,7):\n",
    "            st_m = 'st' + str(st) + \",m\" + str(m)\n",
    "\n",
    "            # Unwrap the fixed probe data versus trolley position\n",
    "            raw_data = trolley_moment_df[['tr_phi', st_m]].copy()\n",
    "            raw_low = raw_data.copy()\n",
    "            raw_high = raw_data.copy()\n",
    "            raw_low['tr_phi'] = raw_low['tr_phi'] - 360\n",
    "            raw_high['tr_phi'] = raw_high['tr_phi'] + 360\n",
    "            unwrap_nomask_df = pd.concat([raw_low, raw_data, raw_high])\n",
    "\n",
    "            unwrap_mask_df = unwrap_nomask_df.copy()\n",
    "            # change mask to exclude 15 degrees\n",
    "            veto_adjust = (15.-7.)/2.\n",
    "            mask = ((unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust) |\n",
    "                    (unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust-360) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust-360) |\n",
    "                    (unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust+360) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust+360))\n",
    "            unwrap_mask_df[st_m] = unwrap_nomask_df[st_m].mask(mask)\n",
    "            unwrap_mask_df['tr_phi'] = unwrap_nomask_df['tr_phi']\n",
    "\n",
    "            unwrap_filled_df = unwrap_mask_df.copy()\n",
    "            temp = unwrap_filled_df.rolling(int(500),win_type='triang',min_periods=1,center=True).mean()\n",
    "            temp = temp.rolling(int(500),win_type='triang',min_periods=1,center=True).mean()\n",
    "            unwrap_filled_df[st_m] = unwrap_filled_df[st_m].mask(mask, temp[st_m])\n",
    "\n",
    "            length = raw_data.shape[0]\n",
    "            filled_df = unwrap_filled_df.iloc[length:2*length,:]\n",
    "\n",
    "            trolley_effect_removed_df[st_m] = filled_df[st_m]\n",
    "\n",
    "    print '\\rFinished removing trolley images from ' + str(length) + ' events.'\n",
    "    return trolley_effect_removed_df\n",
    "\n",
    "station_phi = STATION_BARCODE_PHI\n",
    "station_edges = STATION_BARCODE_EDGES\n",
    "\n",
    "corrected_df_1 = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3956_moment_df'))\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, _ = trolley_run_station_average(corrected_df_1)\n",
    "\n",
    "corrected_df_2 = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_2.h5', key='run_3997_moment_df'))\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, _ = trolley_run_station_average(corrected_df_2)\n",
    "\n",
    "# load all fixed probe runs moment_df into one big moment_df (runs 3959--3994)\n",
    "print 'Appending fixed probe runs.'\n",
    "fp_moment_df = pd.read_hdf('60hr_fixed_probe_runs.h5', key='run_3959_moment_df')\n",
    "pts = fp_moment_df.shape[0]\n",
    "for run in np.arange(3959, 3995):\n",
    "    temp_df = pd.read_hdf('60hr_fixed_probe_runs.h5', key='run_'+str(run)+'_moment_df')\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    fp_moment_df = fp_moment_df.append(temp_df)\n",
    "\n",
    "# load all the trolley runs corrected_df into on big file (to show window of 60hr set) ()\n",
    "print '\\nAppending trolley runs.'\n",
    "tr_corrected_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3955_moment_df'))\n",
    "for run in np.arange(3956, 3959):\n",
    "    temp_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_'+str(run)+'_moment_df'))\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "for run in np.arange(3995, 3999):\n",
    "    temp_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_2.h5', key='run_'+str(run)+'_moment_df'))\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "\n",
    "print '\\nDone appending runs.'\n",
    "\n",
    "### Apply baseline corrections, generate virtual trolley measurements\n",
    "\n",
    "fp_moment_baseline = fp_moment_df.copy()\n",
    "fp_moment_tr_run_baseline = tr_corrected_df.copy()\n",
    "\n",
    "# apply baseline corrections to each fp stm\n",
    "print \"\\nSubtracting fixed probe baselines.\"\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    for m in np.arange(num_probes):\n",
    "        stm = 'st'+str(st)+',m'+str(m+1)\n",
    "\n",
    "        def backwards_correction(time):\n",
    "            c1 = fp_baseline_1[st, m]\n",
    "            c2 = fp_baseline_2[st, m]\n",
    "            t1 = baseline_time_1[st]\n",
    "            t2 = baseline_time_2[st]\n",
    "            return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "\n",
    "        correction = backwards_correction(fp_moment_baseline.index.values)\n",
    "        fp_moment_baseline[stm] = fp_moment_baseline[stm] - correction\n",
    "        correction = backwards_correction(fp_moment_tr_run_baseline.index.values)\n",
    "        fp_moment_tr_run_baseline[stm] = fp_moment_tr_run_baseline[stm] - correction\n",
    "\n",
    "        print '\\rstm: ' + stm + '.',\n",
    "\n",
    "# replace columns in vtr with Jacobian-fixed columns from fp baseline correction\n",
    "\n",
    "print \"\\n\\nApplying Jacobian.\"\n",
    "vtr_df = fp_moment_df.copy()\n",
    "vtr_tr_run_df = tr_corrected_df.copy()\n",
    "\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    if num_probes == 4:\n",
    "        num_moments = 4\n",
    "        if st == 41:\n",
    "            J = trfp.J_4_PROBE_ST41\n",
    "        elif st == 37 | st == 39:\n",
    "            J = trfp.J_4_PROBE_ST37_ST39\n",
    "        else:\n",
    "            J = trfp.J_4_PROBE\n",
    "    else:\n",
    "        num_moments = 5\n",
    "        if st < 7:\n",
    "            J = trfp.J_6_PROBE_OFFSET\n",
    "        else:\n",
    "            J = trfp.J_6_PROBE\n",
    "    # run over each vtr moment:\n",
    "    for m in np.arange(num_moments):\n",
    "        vtr_stm = 'st'+str(st)+',m'+str(m+1)\n",
    "        fp_stm = ['st'+str(st)+',m'+str(fp_m+1) for fp_m in np.arange(num_moments)]\n",
    "        vtr_df[vtr_stm] = fp_moment_baseline[fp_stm].dot(J[m])\n",
    "        vtr_tr_run_df[vtr_stm] = fp_moment_tr_run_baseline[fp_stm].dot(J[m])\n",
    "\n",
    "        print '\\rstm: ' + stm + '.',\n",
    "\n",
    "# Add trolley baseline correction (with backwards correction)\n",
    "print \"\\n\\nAdding trolley baselines.\"\n",
    "\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    for m in np.arange(num_probes):\n",
    "        stm = 'st'+str(st)+',m'+str(m+1)\n",
    "\n",
    "        def backwards_correction(time):\n",
    "            c1 = tr_baseline_1[st, m]\n",
    "            c2 = tr_baseline_2[st, m]\n",
    "            t1 = baseline_time_1[st]\n",
    "            t2 = baseline_time_2[st]\n",
    "            return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "\n",
    "        correction = backwards_correction(vtr_df.index.values)\n",
    "        vtr_df[stm] = vtr_df[stm] + correction\n",
    "        correction = backwards_correction(vtr_tr_run_df.index.values)\n",
    "        vtr_tr_run_df[stm] = vtr_tr_run_df[stm] + correction\n",
    "\n",
    "        print '\\rstm: ' + stm + '.',\n",
    "\n",
    "print '\\n\\nDone.'\n",
    "\n",
    "# bin into the agreed upon bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(1524384055, 1524641055, 1000)-500\n",
    "bin_centers = np.arange(1524384055, 1524640055, 1000)\n",
    "\n",
    "vtr_time_bin_df = vtr_df.groupby(pd.cut(vtr_df.index,bins)).mean()\n",
    "vtr_time_bin_df.index = bin_centers\n",
    "\n",
    "test_df = vtr_time_bin_df.copy()\n",
    "\n",
    "azi_avg_df = pd.DataFrame(np.zeros((test_df.shape[0],6)),\n",
    "                         index = test_df.index,\n",
    "                         columns = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "weight = summed_azimuth_1+summed_azimuth_2\n",
    "weight[1] = 0\n",
    "weight[3] = 0\n",
    "weight[5] = 0\n",
    "weight[54] = 0\n",
    "total_weight = np.sum(weight)\n",
    "\n",
    "for m in np.arange(6):\n",
    "    stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "    azi_avg_df['m'+str(m+1)] = test_df[stm_list].multiply(weight).sum(axis=1)/total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = azi_avg_df[['m1', 'm2', 'm3', 'm5']].copy()\n",
    "\n",
    "review_df['m1_unc'] = 0.116\n",
    "review_df['m2_unc'] = 0.060\n",
    "review_df['m3_unc'] = 0.062\n",
    "review_df['m5_unc'] = 0.100\n",
    "\n",
    "review_df['m1'] = review_df['m1']/61.79\n",
    "review_df['m2'] = review_df['m2']/61.79\n",
    "review_df['m3'] = review_df['m3']/61.79\n",
    "review_df['m5'] = review_df['m5']/61.79\n",
    "\n",
    "review_df = review_df.reindex(sorted(review_df.columns), axis=1)\n",
    "\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = azi_avg_df[['m1', 'm2', 'm3', 'm5']].copy()\n",
    "\n",
    "review_df['m1_unc'] = 0.116\n",
    "review_df['m2_unc'] = 0.060\n",
    "review_df['m3_unc'] = 0.062\n",
    "review_df['m5_unc'] = 0.100\n",
    "\n",
    "review_df['m1'] = review_df['m1']/61.79\n",
    "review_df['m2'] = review_df['m2']/61.79\n",
    "review_df['m3'] = review_df['m3']/61.79\n",
    "review_df['m5'] = review_df['m5']/61.79\n",
    "\n",
    "review_df = review_df.reindex(sorted(review_df.columns), axis=1)\n",
    "\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_df = vtr_time_bin_df.iloc[0].copy()\n",
    "m = ['st'+str(i)+',m1' for i in range(72)]\n",
    "drift_df = drift_df[m]\n",
    "\n",
    "for ii in range(72):\n",
    "    stm = 'st'+str(ii)+',m1'\n",
    "    drift_df[stm] = drift_df[stm] - tr_baseline_1[ii,0]\n",
    "\n",
    "print np.max(drift_df.iloc[0])/61.79\n",
    "print '\\n'\n",
    "\n",
    "print drift_df/61.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
