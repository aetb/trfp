{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/04\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import copy\n",
    "\n",
    "import gm2\n",
    "import trfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_phi = copy.copy(trfp.STATION_BARCODE_PHI)\n",
    "station_edges = copy.copy(trfp.STATION_BARCODE_EDGES)\n",
    "\n",
    "station_edges_6_probe = copy.copy(trfp.STATION_BARCODE_EDGES)\n",
    "\n",
    "# need to step through all stations\n",
    "# if station is a 4-probe station, change both its edge to its phi position\n",
    "\n",
    "for st in range(72):\n",
    "    if len(trfp.STATION_PROBE_ID[st]) == 4:\n",
    "        station_edges_6_probe[st] = station_phi[st]\n",
    "        station_edges_6_probe[st+1] = station_phi[st]\n",
    "station_edges_6_probe[0] = station_phi[71]\n",
    "\n",
    "# print station_edges\n",
    "# print station_edges_6_probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trolley_run_station_average(corrected_df):\n",
    "\n",
    "    # tr_phi is not monotonic, so sort by tr_phi\n",
    "\n",
    "    corrected_df = corrected_df.sort_values(by=['tr_phi'])\n",
    "\n",
    "    measured_phi = corrected_df['tr_phi'].values\n",
    "    measured_extent = (np.roll(measured_phi,-1)-np.roll(measured_phi,1))/2\n",
    "    measured_extent[0] = measured_extent[0]+180\n",
    "    measured_extent[-1] = measured_extent[-1]+180\n",
    "    # print np.max(measured_extent)\n",
    "\n",
    "    corrected_df['tr_extent'] = pd.Series(measured_extent, index=corrected_df.index)\n",
    "    corrected_df = corrected_df.sort_index()\n",
    "\n",
    "    # for a given station:\n",
    "    # create a mask for when trolley is in [low edge, high edge)\n",
    "    tr_baseline = np.full((72,6), np.nan)\n",
    "    fp_baseline = np.full((72,6), np.nan)\n",
    "    summed_azimuth = np.full((72,6), np.nan)\n",
    "    summed_pts = np.full((72,6), np.nan)\n",
    "    baseline_time = np.full((72,6), np.nan)\n",
    "\n",
    "    for st in range(72):\n",
    "        \n",
    "        num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "        # first do m1-4 for all stations\n",
    "        \n",
    "        if station_edges[st+1] > station_edges[st]:\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges[st]) & (corrected_df['tr_phi'] < station_edges[st+1])\n",
    "        else:  # case where we go over the 360 deg line\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges[st]) | (corrected_df['tr_phi'] < station_edges[st+1])\n",
    "\n",
    "        out_df = corrected_df[mask].copy()\n",
    "        summed_pts[st, 0:4] = out_df.shape[0]\n",
    "        summed_azimuth[st, 0:4] = sum(out_df['tr_extent'].values)\n",
    "        baseline_time[st, 0:4] = sum(out_df.index.values)/summed_pts[st]\n",
    "\n",
    "        for m in range(4):\n",
    "            st_id = 'tr,m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                tr_baseline[st, m] = sum(out_df['tr_extent'].values*out_df[st_id].values)/sum(out_df['tr_extent'].values)\n",
    "\n",
    "            st_id = 'st'+str(st)+',m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                fp_baseline[st, m] = np.mean(out_df[st_id])\n",
    "        \n",
    "        if num_probes == 4: continue  # moves to next iteration for 4 probe stations\n",
    "\n",
    "        # next do m5+ for all 6-probe stations\n",
    "\n",
    "        if station_edges[st+1] > station_edges[st]:\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges_6_probe[st]) & (corrected_df['tr_phi'] < station_edges_6_probe[st+1])\n",
    "        else:  # case where we go over the 360 deg line\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges_6_probe[st]) | (corrected_df['tr_phi'] < station_edges_6_probe[st+1])\n",
    "        \n",
    "        out_df = corrected_df[mask].copy()\n",
    "        summed_pts[st, 4:6] = out_df.shape[0]\n",
    "        summed_azimuth_6[st, 4:6] = sum(out_df['tr_extent'].values)\n",
    "        baseline_time_6[st, 4:6] = sum(out_df.index.values)/summed_pts[st]\n",
    "\n",
    "        for m in range(4,6):\n",
    "            st_id = 'tr,m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                tr_baseline[st, m] = sum(out_df['tr_extent'].values*out_df[st_id].values)/sum(out_df['tr_extent'].values)\n",
    "\n",
    "            st_id = 'st'+str(st)+',m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                fp_baseline[st, m] = np.mean(out_df[st_id])\n",
    "    \n",
    "    return tr_baseline, fp_baseline, baseline_time, summed_azimuth, summed_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trolley_effect(trolley_moment_df):\n",
    "    '''DOC STRING'''\n",
    "    barcode = station_phi\n",
    "    \n",
    "    veto_extent = 25\n",
    "\n",
    "    trolley_effect_removed_df = trolley_moment_df.copy()\n",
    "\n",
    "    for st in range(72):\n",
    "        print '\\rRemoving trolley image from station ' + str(st) + '.',\n",
    "        for m in range(1,7):\n",
    "            st_m = 'st' + str(st) + \",m\" + str(m)\n",
    "\n",
    "            # Unwrap the fixed probe data versus trolley position\n",
    "            raw_data = trolley_moment_df[['tr_phi', st_m]].copy()\n",
    "            raw_low = raw_data.copy()\n",
    "            raw_high = raw_data.copy()\n",
    "            raw_low['tr_phi'] = raw_low['tr_phi'] - 360\n",
    "            raw_high['tr_phi'] = raw_high['tr_phi'] + 360\n",
    "            unwrap_nomask_df = pd.concat([raw_low, raw_data, raw_high])\n",
    "\n",
    "            unwrap_mask_df = unwrap_nomask_df.copy()\n",
    "            veto_adjust = (veto_extent-7)/2\n",
    "            mask = ((unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust) |\n",
    "                    (unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust-360) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust-360) |\n",
    "                    (unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust+360) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust+360))\n",
    "            unwrap_mask_df[st_m] = unwrap_nomask_df[st_m].mask(mask)\n",
    "            unwrap_mask_df['tr_phi'] = unwrap_nomask_df['tr_phi']\n",
    "\n",
    "            unwrap_filled_df = unwrap_mask_df.copy()\n",
    "            temp = unwrap_filled_df.rolling(int(500),win_type='triang',min_periods=1,center=True).mean()\n",
    "            temp = temp.rolling(int(500),win_type='triang',min_periods=1,center=True).mean()\n",
    "            unwrap_filled_df[st_m] = unwrap_filled_df[st_m].mask(mask, temp[st_m])\n",
    "\n",
    "            length = raw_data.shape[0]\n",
    "            filled_df = unwrap_filled_df.iloc[length:2*length,:]\n",
    "\n",
    "            trolley_effect_removed_df[st_m] = filled_df[st_m]\n",
    "\n",
    "    print '\\rFinished removing trolley images from ' + str(length) + ' events.'\n",
    "    return trolley_effect_removed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished removing trolley images from 4382 events.                                                            \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (6) into shape (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0fdaffdcb844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorrected_df_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_trolley_effect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'60hr_trolley_runs_1.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run_3956_moment_df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtr_baseline_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_baseline_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_time_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummed_azimuth_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_time_6_probe_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummed_azimuth_6_probe_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrolley_run_station_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_df_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorrected_df_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_trolley_effect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'60hr_trolley_runs_2.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run_3997_moment_df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtr_baseline_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_baseline_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_time_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummed_azimuth_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_time_6_probe_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummed_azimuth_6_probe_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrolley_run_station_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_df_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c534ac69f8c8>\u001b[0m in \u001b[0;36mtrolley_run_station_average\u001b[0;34m(corrected_df)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0msummed_pts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0msummed_azimuth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tr_extent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mbaseline_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msummed_pts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (6) into shape (4)"
     ]
    }
   ],
   "source": [
    "corrected_df_1 = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3956_moment_df'))\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, baseline_time_6_probe_1, summed_azimuth_6_probe_1 = trolley_run_station_average(corrected_df_1)\n",
    "\n",
    "corrected_df_2 = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_2.h5', key='run_3997_moment_df'))\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, baseline_time_6_probe_2, summed_azimuth_6_probe_2 = trolley_run_station_average(corrected_df_2)\n",
    "\n",
    "# load all fixed probe runs moment_df into one big moment_df (runs 3959--3994)\n",
    "print 'Appending fixed probe runs.'\n",
    "fp_moment_df = pd.read_hdf('60hr_fixed_probe_runs.h5', key='run_3959_moment_df')\n",
    "pts = fp_moment_df.shape[0]\n",
    "for run in np.arange(3959, 3995):\n",
    "    temp_df = pd.read_hdf('60hr_fixed_probe_runs.h5', key='run_'+str(run)+'_moment_df')\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    fp_moment_df = fp_moment_df.append(temp_df)\n",
    "\n",
    "# load all the trolley runs corrected_df into on big file (to show window of 60hr set) ()\n",
    "print '\\nAppending trolley runs.'\n",
    "tr_corrected_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3955_moment_df'))\n",
    "for run in np.arange(3956, 3959):\n",
    "    temp_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_'+str(run)+'_moment_df'))\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "for run in np.arange(3995, 3999):\n",
    "    temp_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_2.h5', key='run_'+str(run)+'_moment_df'))\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "\n",
    "print '\\nDone appending runs.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply baseline corrections, generate virtual trolley measurements\n",
    "\n",
    "fp_moment_baseline = fp_moment_df.copy()\n",
    "fp_moment_tr_run_baseline = tr_corrected_df.copy()\n",
    "\n",
    "# apply baseline corrections to each fp stm\n",
    "print \"\\nSubtracting fixed probe baselines.\"\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    for m in np.arange(num_probes):\n",
    "        stm = 'st'+str(st)+',m'+str(m+1)\n",
    "\n",
    "        def backwards_correction(time):\n",
    "            c1 = fp_baseline_1[st, m]\n",
    "            c2 = fp_baseline_2[st, m]\n",
    "            if m < 4:\n",
    "                t1 = baseline_time_1[st]\n",
    "                t2 = baseline_time_2[st]\n",
    "            else:\n",
    "                t1 = baseline_time_6_probe_1[st]\n",
    "                t2 = baseline_time_6_probe_2[st]\n",
    "            return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "\n",
    "        correction = backwards_correction(fp_moment_baseline.index.values)\n",
    "        fp_moment_baseline[stm] = fp_moment_baseline[stm] - correction\n",
    "        correction = backwards_correction(fp_moment_tr_run_baseline.index.values)\n",
    "        fp_moment_tr_run_baseline[stm] = fp_moment_tr_run_baseline[stm] - correction\n",
    "\n",
    "        print '\\rstm: ' + stm + '.',\n",
    "\n",
    "# replace columns in vtr with Jacobian-fixed columns from fp baseline correction\n",
    "\n",
    "print \"\\n\\nApplying Jacobian.\"\n",
    "vtr_df = fp_moment_df.copy()\n",
    "vtr_tr_run_df = tr_corrected_df.copy()\n",
    "\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    if num_probes == 4:\n",
    "        num_moments = 4\n",
    "        if st == 41:\n",
    "            J = trfp.J_4_PROBE_ST41\n",
    "        elif st == 37 | st == 39:\n",
    "            J = trfp.J_4_PROBE_ST37_ST39\n",
    "        else:\n",
    "            J = trfp.J_4_PROBE\n",
    "    else:\n",
    "        num_moments = 5\n",
    "        if st < 7:\n",
    "            J = trfp.J_6_PROBE_OFFSET\n",
    "        else:\n",
    "            J = trfp.J_6_PROBE\n",
    "    # run over each vtr moment:\n",
    "    for m in np.arange(num_moments):\n",
    "        vtr_stm = 'st'+str(st)+',m'+str(m+1)\n",
    "        fp_stm = ['st'+str(st)+',m'+str(fp_m+1) for fp_m in np.arange(num_moments)]\n",
    "        vtr_df[vtr_stm] = fp_moment_baseline[fp_stm].dot(J[m])\n",
    "        vtr_tr_run_df[vtr_stm] = fp_moment_tr_run_baseline[fp_stm].dot(J[m])\n",
    "\n",
    "        print '\\rstm: ' + stm + '.',\n",
    "\n",
    "# Add trolley baseline correction (with backwards correction)\n",
    "print \"\\n\\nAdding trolley baselines.\"\n",
    "\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    for m in np.arange(num_probes):\n",
    "        stm = 'st'+str(st)+',m'+str(m+1)\n",
    "\n",
    "        def backwards_correction(time):\n",
    "            c1 = tr_baseline_1[st, m]\n",
    "            c2 = tr_baseline_2[st, m]\n",
    "            if m < 4:\n",
    "                t1 = baseline_time_1[st]\n",
    "                t2 = baseline_time_2[st]\n",
    "            else:\n",
    "                t1 = baseline_time_6_probe_1[st]\n",
    "                t2 = baseline_time_6_probe_2[st]\n",
    "            return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "\n",
    "        correction = backwards_correction(vtr_df.index.values)\n",
    "        vtr_df[stm] = vtr_df[stm] + correction\n",
    "        correction = backwards_correction(vtr_tr_run_df.index.values)\n",
    "        vtr_tr_run_df[stm] = vtr_tr_run_df[stm] + correction\n",
    "\n",
    "        print '\\rstm: ' + stm + '.',\n",
    "\n",
    "print '\\n\\nDone.'\n",
    "\n",
    "# bin into the agreed upon bins\n",
    "\n",
    "bins = np.arange(1524384055, 1524641055, 1000)-500\n",
    "bin_centers = np.arange(1524384055, 1524640055, 1000)\n",
    "\n",
    "    # bins = np.arange(1524383560, 1524640850, 10)-5\n",
    "# bin_centers = np.arange(1524383560, 1524640840, 10)\n",
    "\n",
    "vtr_time_bin_df = vtr_df.groupby(pd.cut(vtr_df.index,bins)).mean()\n",
    "vtr_time_bin_df.index = bin_centers\n",
    "vtr_time_bin_df.head()\n",
    "\n",
    "\n",
    "# print vtr_time_bin_df.iloc[[0,-1]]\n",
    "# print vtr_time_bin_df.shape\n",
    "# print bin_centers.shape\n",
    "\n",
    "test_df = vtr_time_bin_df.copy()\n",
    "\n",
    "azi_avg_df = pd.DataFrame(np.zeros((test_df.shape[0],6)),\n",
    "                         index = test_df.index,\n",
    "                         columns = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "weight = summed_azimuth_1+summed_azimuth_2\n",
    "total_weight = np.sum(weight)\n",
    "for m in range(4):\n",
    "    stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "    azi_avg_df['m'+str(m+1)] = test_df[stm_list].multiply(weight).sum(axis=1)/total_weight\n",
    "\n",
    "weight = summed_azimuth_6_probe_1+summed_azimuth_6_probe_2\n",
    "total_weight = np.sum(weight)\n",
    "for m in range(4,6):\n",
    "    stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "    azi_avg_df['m'+str(m+1)] = test_df[stm_list].multiply(weight).sum(axis=1)/total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azi_avg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = azi_avg_df[['m1', 'm2', 'm3', 'm5']].copy()\n",
    "\n",
    "review_df['m1_unc'] = 0.116\n",
    "review_df['m2_unc'] = 0.060\n",
    "review_df['m3_unc'] = 0.062\n",
    "review_df['m5_unc'] = 0.100\n",
    "\n",
    "review_df['m1'] = review_df['m1']/61.79\n",
    "review_df['m2'] = review_df['m2']/61.79\n",
    "review_df['m3'] = review_df['m3']/61.79\n",
    "review_df['m5'] = review_df['m5']/61.79\n",
    "\n",
    "review_df = review_df.reindex(sorted(review_df.columns), axis=1)\n",
    "\n",
    "\n",
    "# review_df.to_csv('purcell_may27_fixed_m5.csv', float_format='%0.4f', index_label='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters (maybe define standard functions based on these reqs later?)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)     \n",
    "matplotlib.rc('ytick', labelsize=8)\n",
    "matplotlib.rc('axes', titlesize=16)\n",
    "matplotlib.rc('axes', labelsize=12)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "single_column_small = (6.202, 3.833)  #inches\n",
    "single_column_med = (6.202, 6.202)\n",
    "single_column_large = (6.202, 7.666)\n",
    "\n",
    "def plt_unix_time_to_CST(ax):\n",
    "    plt.locator_params(axis='x', nbins=5)\n",
    "    xticks = ax.get_xticks()\n",
    "    ax.set_xticklabels([pd.to_datetime(tm, unit='s').tz_localize('UTC').tz_convert('US/Central').strftime('%Y-%m-%d\\n %H:%M:%S %Z')\n",
    "                          for tm in xticks], rotation=30)\n",
    "\n",
    "def plt_set_labels(ax, x_label, y_label, title):\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# fig:final_nsext\n",
    "fig4,ax4 = plt.subplots(1,1)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.plot(azi_avg_df.index.values, azi_avg_df['m5'], '.', color='navy', markersize=2)\n",
    "# ax4.set_ylim(-60,-40)\n",
    "plt_unix_time_to_CST(ax4)\n",
    "plt_set_labels(ax4, '', '$m_3$ (Hz)', 'Azimuthally Averaged Normal Sextupole')\n",
    "\n",
    "fig4.set_size_inches(single_column_small)\n",
    "fig4.tight_layout()\n",
    "fig4.savefig('final_nsext.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
