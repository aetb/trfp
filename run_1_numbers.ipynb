{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import gm2\n",
    "import trfp\n",
    "import plotting_functions as plt2\n",
    "import helper_functions as helper\n",
    "\n",
    "import field_map_config_run1 as map_config\n",
    "import muon_dist_config_run1 as dist_config\n",
    "\n",
    "\n",
    "def parameterize_beam(beam, beam_x, beam_y):\n",
    "    x, y = np.meshgrid(beam_x, beam_y)\n",
    "\n",
    "    dx = np.mean(np.diff(beam_x))\n",
    "    dy = np.mean(np.diff(beam_y))\n",
    "\n",
    "    order = trfp.matrices._MULTIPOLE_ORDER\n",
    "    skew = trfp.matrices._MULTIPOLE_SKEW\n",
    "    multipole = trfp.matrices.__multipole\n",
    "\n",
    "    n = len(order)\n",
    "    k = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        f = multipole(order[i], skew[i], 1, x, y)\n",
    "        k[i] = np.sum(f*beam*dx*dy)/np.sum(beam*dx*dy)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ctags', 'km_total', 'BB_total',\n",
    "           'k1', 'm1', 'km1', 'BB1',\n",
    "           'k2', 'm2', 'km2', 'BB2',\n",
    "           'k3', 'm3', 'km3', 'BB3',\n",
    "           'k4', 'm4', 'km4', 'BB4',\n",
    "           'k5', 'm5', 'km5', 'BB5',\n",
    "           'k6', 'm6', 'km6', 'BB6',\n",
    "           'k7', 'm7', 'km7', 'BB7',\n",
    "           'k8', 'm8', 'km8', 'BB8',\n",
    "           'k9', 'm9', 'km9', 'BB9']\n",
    "\n",
    "index = map_config.subruns\n",
    "\n",
    "run_1_df = pd.DataFrame(0.0, index=index, columns=columns)\n",
    "\n",
    "data_file = '/data2/aetb/2020-09-30_hybrid_maps.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for run in index:\n",
    "    \n",
    "    print 'Starting Run ' + run\n",
    "\n",
    "    key = 'run_'+run\n",
    "\n",
    "    vtm_df = pd.read_hdf(data_file, key=key)\n",
    "\n",
    "    config_dict = dist_config.config_dict\n",
    "    data_run = run\n",
    "\n",
    "    vtm_file = config_dict[data_run][0]\n",
    "    vtm_key = config_dict[data_run][1]\n",
    "    interp_file = config_dict[data_run][2]\n",
    "    interp_key_1 = config_dict[data_run][3]\n",
    "    interp_key_2 = config_dict[data_run][4]\n",
    "    gold_subruns_file = config_dict[data_run][5]\n",
    "    beam_files = config_dict[data_run][6]\n",
    "\n",
    "    tr_interp_df_1 = pd.read_hdf(interp_file, key=interp_key_1)\n",
    "    t_start = np.mean(tr_interp_df_1.index.values)\n",
    "    tr_interp_df_2 = pd.read_hdf(interp_file, key=interp_key_2)\n",
    "    if data_run == '1d6': t_end = np.inf  # deals with not having a closing trolley run for 1d6\n",
    "    else: t_end = np.mean(tr_interp_df_2.index.values)\n",
    "\n",
    "    # Import a gold subrun list\n",
    "\n",
    "    subrun_df = pd.read_hdf(interp_file, key='subrun_df')\n",
    "    gold_subruns = np.loadtxt(gold_subruns_file)\n",
    "    gold_subruns_df = pd.DataFrame(gold_subruns.astype(int), columns=['run', 'subrun']).merge(subrun_df, on=['run', 'subrun'])\n",
    "    gold_subruns_df = gold_subruns_df[(gold_subruns_df['start_gps']>=t_start) & (gold_subruns_df['end_gps']<=t_end)]\n",
    "\n",
    "    # need to bin into subruns\n",
    "\n",
    "    vtm_interp = interp1d(vtm_df.index, vtm_df.values, axis=0)\n",
    "    times = gold_subruns_df['start_gps'].append(gold_subruns_df['end_gps'])\n",
    "\n",
    "    boundary_df = pd.DataFrame(vtm_interp(times), index=times, columns=vtm_df.columns)\n",
    "\n",
    "    vtm_interp_df = vtm_df.append(boundary_df).sort_index()\n",
    "\n",
    "    boundary_cut = pd.IntervalIndex.from_arrays(gold_subruns_df['start_gps'], gold_subruns_df['end_gps'], closed='both')\n",
    "    vtm_cut = pd.cut(vtm_interp_df.index, boundary_cut)\n",
    "\n",
    "    def avg_technique(bin_):\n",
    "        numer = np.trapz(bin_, x=bin_.index.values, axis=0)\n",
    "        denom = np.max(bin_.index.values)-np.min(bin_.index.values)\n",
    "\n",
    "        return numer/denom\n",
    "\n",
    "    avg_field = vtm_interp_df.groupby(vtm_cut).apply(avg_technique)\n",
    "    avg_df = pd.DataFrame.from_dict(dict(zip(avg_field.index, avg_field.values)), orient='index', columns=vtm_df.columns)\n",
    "    avg_df['start_gps'] = [interval[0] for interval in avg_df.index.to_tuples().values]\n",
    "    avg_df['end_gps'] = [interval[1] for interval in avg_df.index.to_tuples().values]\n",
    "\n",
    "    output_df = gold_subruns_df[['run', 'subrun', 'start_gps', 'end_gps', 'ctags']].copy()\n",
    "    output_df = output_df.merge(avg_df, on=['start_gps', 'end_gps'])\n",
    "\n",
    "    output_columns = ['run', 'subrun', 'start_gps', 'end_gps', 'ctags'] + ['st'+str(st)+',m'+str(m+1) for st in range(72) for m in range(9)]\n",
    "    output_df = output_df[output_columns]\n",
    "\n",
    "    ###################################################\n",
    "    ## make a time varying, azimuthally varying profile\n",
    "    ###################################################\n",
    "\n",
    "    azi_run_k_df = pd.DataFrame()\n",
    "\n",
    "    for ii in range(len(beam_files)):\n",
    "\n",
    "        beam_filename = 'BeamSpot_'+str(beam_files[ii][0])+'_'+str(beam_files[ii][1])+'.root'\n",
    "\n",
    "        beam, beam_x, beam_y, phi = gm2.trfp.conv.loadBeamNew(beam_filename, path='/data1/newg2/Run1TrackerData/22Sep2020/', integrated=False)\n",
    "        phi = phi + 18.35\n",
    "        beam_x = beam_x/10\n",
    "        beam_y = beam_y/10\n",
    "        for hh in range(len(beam)):\n",
    "            beam[hh] = np.transpose(beam[hh])\n",
    "\n",
    "        k_azi = np.empty([14, 72])\n",
    "        for ll in range(72):\n",
    "            k_azi[:,ll] = parameterize_beam(beam[ll], beam_x, beam_y)\n",
    "\n",
    "        k_azi_interp = np.empty([14,72*3])\n",
    "        k_azi_interp[:,0:72] = k_azi\n",
    "        k_azi_interp[:,72:144] = k_azi\n",
    "        k_azi_interp[:,144:216] = k_azi\n",
    "\n",
    "        phi_interp = np.array([phi-360, phi, phi+360]).flatten()\n",
    "\n",
    "        k_interp = interp1d(phi_interp, k_azi_interp, axis=1, kind='cubic')\n",
    "\n",
    "        station_centers = (trfp.geometry.STATION_BARCODE_EDGES[:-1] + trfp.geometry.STATION_BARCODE_EDGES[1:])/2\n",
    "        station_centers[2] = station_centers[2] + 180\n",
    "\n",
    "        k_station = k_interp(station_centers)\n",
    "\n",
    "        run_range = np.arange(beam_files[ii][0], beam_files[ii][1]+1)\n",
    "\n",
    "        for jj in range(len(run_range)):\n",
    "            run_append_dict = {}\n",
    "            run_append_dict['run'] = run_range[jj]\n",
    "            for st in range(72):    \n",
    "                for kk in range(9):\n",
    "                    run_append_dict['st'+str(st)+',k'+str(kk+1)] = [k_station[kk,st]]\n",
    "            azi_run_k_df = azi_run_k_df.append(pd.DataFrame.from_dict(run_append_dict))\n",
    "\n",
    "    stk = ['st'+str(st)+',k'+str(k+1) for st in range(72) for k in range(9)]\n",
    "    azi_run_k_df = azi_run_k_df.set_index(azi_run_k_df['run'].values)\n",
    "    azi_run_k_df = azi_run_k_df[stk]\n",
    "\n",
    "    ## need to add in a run 15954 for Run 1a\n",
    "\n",
    "    if run == '1a1':\n",
    "        append_df = ((azi_run_k_df.loc[15953] + azi_run_k_df.loc[15955])/2)\n",
    "        append_df.name = 15954\n",
    "        azi_run_k_df = azi_run_k_df.append(append_df)\n",
    "        azi_run_k_df = azi_run_k_df.sort_index()\n",
    "\n",
    "    stkm = ['st'+str(st)+',km'+str(km+1) for st in range(72) for km in range(9)]\n",
    "    stk = ['st'+str(st)+',k'+str(k+1) for st in range(72) for k in range(9)]\n",
    "    stm = ['st'+str(st)+',m'+str(m+1) for st in range(72) for m in range(9)]\n",
    "\n",
    "    def subrun_bin(df):\n",
    "        stm = ['st'+str(st)+',m'+str(m+1) for st in range(72) for m in range(9)]\n",
    "\n",
    "        bin_df = pd.Series(index=['start_gps', 'end_gps', 'ctags']+stm)\n",
    "        bin_df['start_gps'] = df['start_gps'].min()\n",
    "        bin_df['end_gps'] = df['end_gps'].max()\n",
    "        bin_df['ctags'] = df['ctags'].sum()\n",
    "        bin_df[stm] = df['ctags'].dot(df[stm])/df['ctags'].sum()\n",
    "\n",
    "        return bin_df\n",
    "\n",
    "    bin_df = output_df.groupby('run').apply(subrun_bin)\n",
    "\n",
    "    run_df = (azi_run_k_df[stk].rename(columns=dict(zip(stk,stm))) * bin_df[stm]).dropna()\n",
    "    run_df = run_df.rename(columns=dict(zip(stm,stkm)))\n",
    "    run_df['start_gps'] = bin_df['start_gps']\n",
    "    run_df['end_gps'] = bin_df['end_gps']\n",
    "    run_df['ctags'] = bin_df['ctags']\n",
    "    run_df = run_df[['start_gps', 'end_gps', 'ctags']+stkm]\n",
    "\n",
    "    weight = (trfp.STATION_BARCODE_EDGES[1:] - trfp.STATION_BARCODE_EDGES[:-1])\n",
    "    weight[2] = (weight[2] + 360)\n",
    "    weight = weight/360\n",
    "\n",
    "    run_sum = run_df['ctags'].dot(run_df[stkm])/run_df['ctags'].sum()\n",
    "\n",
    "    run_km = np.empty(9)\n",
    "    for km in range(9):\n",
    "        _stkm = ['st'+str(st)+',km'+str(km+1) for st in range(72)]\n",
    "        run_km[km] = run_sum[_stkm].multiply(weight).sum()\n",
    "\n",
    "\n",
    "    # print np.round(run_km[0], decimals=5)\n",
    "    # print np.round(run_km[1], decimals=5)\n",
    "    # print np.round(run_km[2], decimals=5)\n",
    "    # print np.round(run_km[3], decimals=5)\n",
    "    # print np.round(run_km[4], decimals=5)\n",
    "    # print\n",
    "    # print np.round(np.sum(run_km), decimals=5)\n",
    "\n",
    "    for m in range(9):\n",
    "\n",
    "        # get azi and time averaged m values\n",
    "        stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "        bin_df['m'+str(m+1)] = bin_df[stm_list].multiply(weight).sum(axis=1)\n",
    "        run_1_df['m'+str(m+1)].loc[run] = (bin_df['ctags']*bin_df['m'+str(m+1)]).sum()/bin_df['ctags'].sum()\n",
    "\n",
    "        stk_list = ['st'+str(st)+',k'+str(m+1) for st in np.arange(72)]\n",
    "        azi_run_k_df['k'+str(m+1)] = azi_run_k_df[stk_list].multiply(weight).sum(axis=1)\n",
    "        run_1_df['k'+str(m+1)].loc[run] = (bin_df['ctags']*azi_run_k_df['k'+str(m+1)]).sum()/bin_df['ctags'].sum()\n",
    "\n",
    "        stkm_list = ['st'+str(st)+',km'+str(m+1) for st in np.arange(72)]\n",
    "        run_df['km'+str(m+1)] = run_df[stkm_list].multiply(weight).sum(axis=1)\n",
    "        run_1_df['km'+str(m+1)].loc[run] = (run_df['ctags']*run_df['km'+str(m+1)]).sum()/run_df['ctags'].sum()\n",
    "\n",
    "    run_1_df['ctags'].loc[run] = run_df['ctags'].sum()\n",
    "\n",
    "    run_1_df['km_total'].loc[run] = run_1_df[['km'+str(km+1) for km in range(9)]].loc[run].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_1_df.to_csv('run_1_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print run_1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk = ['st'+str(st)+',k3' for st in range(72)]\n",
    "\n",
    "print np.mean(azi_run_k_df[stk].multiply(weight).sum(axis=1)/360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_km[2]/np.mean(azi_run_k_df[stk].multiply(weight).sum(axis=1)/360)*16  # *0.0222*16/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
