{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/04\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import gm2\n",
    "import trfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending fixed probe runs.\n",
      "Appending run 3994.                              \n",
      "Appending trolley runs.\n",
      "Appending run 3998.   \n",
      "Blinding trolley runs.\n",
      "\n",
      "Done appending runs.\n",
      "\n",
      "Subtracting fixed probe baselines.\n",
      "stm: st71,m4.   \n",
      "\n",
      "Applying Jacobian.\n",
      "stm: st71,m4.        \n",
      "\n",
      "Adding trolley baselines.\n",
      "stm: st71,m4.   \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Blind trolley and fixed probe separately, but blind all fixed probe stations the same\n",
    "# Blind dipole by order 1 ppm = 61.79 Hz\n",
    "# Blind higher moments by order 100 ppb = 6.179 Hz\n",
    "tr_blinds = np.zeros(17)\n",
    "# tr_blinds[0] = np.random.uniform(-61.79, 61.79)\n",
    "# tr_blinds[1:] = np.random.uniform(-6.179, 6.179, 16)\n",
    "fp_blinds = np.zeros(6)\n",
    "# fp_blinds[0] = np.random.uniform(-61.79, 61.79)\n",
    "# fp_blinds[1:] = np.random.uniform(-6.179, 6.179, 5)\n",
    "\n",
    "def blind_tr_moment_df(moment_df, fp_blinds, tr_blinds):\n",
    "    out_df = moment_df.copy()\n",
    "    out_df['tr,m1'] = moment_df['tr,m1'] + tr_blinds[0]\n",
    "    for st in np.arange(72):\n",
    "        stm = 'st' + str(st) + ',m1'\n",
    "        out_df[stm] = moment_df[stm] + fp_blinds[0]\n",
    "    for m in np.arange(2,7):\n",
    "        trm = 'tr,m' + str(m)\n",
    "        out_df[trm] = moment_df[trm] + tr_blinds[m-1]\n",
    "        for st in np.arange(72):\n",
    "            stm = 'st' + str(st) + ',m' + str(m)\n",
    "            out_df[stm] = moment_df[stm] + fp_blinds[m-1]\n",
    "\n",
    "    # Blind higher order trolley moments by order 100 ppb = 6.179 Hz\n",
    "    for m in np.arange(7,18):\n",
    "        trm = 'tr,m' + str(m)\n",
    "        out_df[trm] = moment_df[trm] + tr_blinds[m-1]\n",
    "        \n",
    "    return out_df\n",
    "\n",
    "def blind_fp_moment_df(moment_df, fp_blinds):\n",
    "    # Blind trolley and fixed probe separately, but blind all fixed probe stations the same\n",
    "    # Blind dipole by order 1 ppm = 61.79 Hz\n",
    "    out_df = moment_df.copy()\n",
    "    for st in np.arange(72):\n",
    "        stm = 'st' + str(st) + ',m1'\n",
    "        out_df[stm] = moment_df[stm] + fp_blinds[0]\n",
    "    \n",
    "    # Blind quads, sexts, m6 by order 100 ppb = 6.179 Hz\n",
    "    for m in np.arange(2,7):\n",
    "        for st in np.arange(72):\n",
    "            stm = 'st' + str(st) + ',m' + str(m)\n",
    "            out_df[stm] = moment_df[stm] + fp_blinds[m-1]\n",
    "        \n",
    "    return out_df\n",
    "\n",
    "station_phi = trfp.STATION_BARCODE_PHI\n",
    "station_edges = trfp.STATION_BARCODE_EDGES\n",
    "\n",
    "path = '../run_hdf5/60hr/'\n",
    "\n",
    "corrected_df_1 = pd.read_hdf(path+'60hr_trolley_runs_1.h5', key='run_3956_corrected_df')\n",
    "# corrected_df_1 = blind_tr_moment_df(corrected_df_1, fp_blinds, tr_blinds)\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, _ = trfp.trolley_run_station_average(corrected_df_1)\n",
    "\n",
    "corrected_df_2 = pd.read_hdf(path+'60hr_trolley_runs_2.h5', key='run_3997_corrected_df')\n",
    "# corrected_df_2 = blind_tr_moment_df(corrected_df_2, fp_blinds, tr_blinds)\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, _ = trfp.trolley_run_station_average(corrected_df_2)\n",
    "\n",
    "# load all fixed probe runs moment_df into one big moment_df (runs 3959--3994)\n",
    "print 'Appending fixed probe runs.'\n",
    "fp_moment_df = pd.read_hdf(path+'60hr_fixed_probe_runs.h5', key='run_3959_moment_df')\n",
    "pts = fp_moment_df.shape[0]\n",
    "for run in np.arange(3959, 3995):\n",
    "    temp_df = pd.read_hdf(path+'60hr_fixed_probe_runs.h5', key='run_'+str(run)+'_moment_df')\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    fp_moment_df = fp_moment_df.append(temp_df)\n",
    "    \n",
    "# print '\\nBlinding fixed probe runs.'\n",
    "# fp_moment_df = blind_fp_moment_df(fp_moment_df, fp_blinds)\n",
    "\n",
    "# load all the trolley runs corrected_df into on big file (to show window of 60hr set) ()\n",
    "print '\\nAppending trolley runs.'\n",
    "tr_corrected_df = pd.read_hdf(path+'60hr_trolley_runs_1.h5', key='run_3955_corrected_df')\n",
    "for run in np.arange(3956, 3959):\n",
    "    temp_df = pd.read_hdf(path+'60hr_trolley_runs_1.h5', key='run_'+str(run)+'_corrected_df')\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "for run in np.arange(3995, 3999):\n",
    "    temp_df = pd.read_hdf(path+'60hr_trolley_runs_2.h5', key='run_'+str(run)+'_corrected_df')\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "\n",
    "print '\\nBlinding trolley runs.'\n",
    "# tr_corrected_df = blind_tr_moment_df(tr_corrected_df, fp_blinds, tr_blinds)\n",
    "\n",
    "print '\\nDone appending runs.'\n",
    "\n",
    "### Apply baseline corrections, generate virtual trolley measurements\n",
    "\n",
    "fp_moment_baseline = fp_moment_df.copy()\n",
    "fp_moment_tr_run_baseline = tr_corrected_df.copy()\n",
    "\n",
    "# apply baseline corrections to each fp stm\n",
    "print \"\\nSubtracting fixed probe baselines.\"\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    for m in np.arange(num_probes):\n",
    "        stm = 'st'+str(st)+',m'+str(m+1)\n",
    "        \n",
    "        def backwards_correction(time):\n",
    "            c1 = fp_baseline_1[st, m]\n",
    "            c2 = fp_baseline_2[st, m]\n",
    "            t1 = baseline_time_1[st]\n",
    "            t2 = baseline_time_2[st]\n",
    "            return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "        \n",
    "        correction = backwards_correction(fp_moment_baseline.index.values)\n",
    "        fp_moment_baseline[stm] = fp_moment_baseline[stm] - correction\n",
    "        correction = backwards_correction(fp_moment_tr_run_baseline.index.values)\n",
    "        fp_moment_tr_run_baseline[stm] = fp_moment_tr_run_baseline[stm] - correction\n",
    "        \n",
    "        print '\\rstm: ' + stm + '.',\n",
    "        \n",
    "# replace columns in vtr with Jacobian-fixed columns from fp baseline correction\n",
    "\n",
    "print \"\\n\\nApplying Jacobian.\"\n",
    "vtr_df = fp_moment_df.copy()\n",
    "vtr_tr_run_df = tr_corrected_df.copy()\n",
    "\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    if num_probes == 4:\n",
    "        num_moments = 4\n",
    "        if st == 41:\n",
    "            J = trfp.J_4_PROBE_ST41\n",
    "        elif st == 37 | st == 39:\n",
    "            J = trfp.J_4_PROBE_ST37_ST39\n",
    "        else:\n",
    "            J = trfp.J_4_PROBE\n",
    "    else:\n",
    "        num_moments = 5\n",
    "        if st < 7:\n",
    "            J = trfp.J_6_PROBE_OFFSET\n",
    "        else:\n",
    "            J = trfp.J_6_PROBE\n",
    "    # run over each vtr moment:\n",
    "    for m in np.arange(num_moments):\n",
    "        vtr_stm = 'st'+str(st)+',m'+str(m+1)\n",
    "        fp_stm = ['st'+str(st)+',m'+str(fp_m+1) for fp_m in np.arange(num_moments)]\n",
    "        vtr_df[vtr_stm] = fp_moment_baseline[fp_stm].dot(J[m])\n",
    "        vtr_tr_run_df[vtr_stm] = fp_moment_tr_run_baseline[fp_stm].dot(J[m])\n",
    "        \n",
    "        print '\\rstm: ' + stm + '.',\n",
    "        \n",
    "# Add trolley baseline correction (with backwards correction)\n",
    "print \"\\n\\nAdding trolley baselines.\"\n",
    "\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    for m in np.arange(num_probes):\n",
    "        stm = 'st'+str(st)+',m'+str(m+1)\n",
    "        \n",
    "        def backwards_correction(time):\n",
    "            c1 = tr_baseline_1[st, m]\n",
    "            c2 = tr_baseline_2[st, m]\n",
    "            t1 = baseline_time_1[st]\n",
    "            t2 = baseline_time_2[st]\n",
    "            return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "        \n",
    "        correction = backwards_correction(vtr_df.index.values)\n",
    "        vtr_df[stm] = vtr_df[stm] + correction\n",
    "        correction = backwards_correction(vtr_tr_run_df.index.values)\n",
    "        vtr_tr_run_df[stm] = vtr_tr_run_df[stm] + correction\n",
    "        \n",
    "        print '\\rstm: ' + stm + '.',\n",
    "\n",
    "print '\\n\\nDone.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin into the agreed upon bins\n",
    "\n",
    "bins = np.arange(1524383560, 1524640850, 10)-5\n",
    "bin_centers = np.arange(1524383560, 1524640840, 10)\n",
    "\n",
    "vtr_time_bin_df = vtr_df.groupby(pd.cut(vtr_df.index,bins)).mean()\n",
    "vtr_time_bin_df.index = bin_centers\n",
    "\n",
    "# print vtr_time_bin_df.iloc[[0,-1]]\n",
    "# print vtr_time_bin_df.shape\n",
    "# print bin_centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = vtr_time_bin_df.copy()\n",
    "\n",
    "azi_avg_df = pd.DataFrame(np.zeros((test_df.shape[0],6)),\n",
    "                         index = test_df.index,\n",
    "                         columns = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "weight = summed_azimuth_1+summed_azimuth_2\n",
    "total_weight = np.sum(summed_azimuth_1 + summed_azimuth_2)\n",
    "\n",
    "for m in np.arange(6):\n",
    "    stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "    azi_avg_df['m'+str(m+1)] = test_df[stm_list].multiply(weight).sum(axis=1)/total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    m1  m1_unc        m2  m2_unc        m3  m3_unc        m5  \\\n",
      "1524383560  841.864960   0.116  0.086612    0.06  0.169908   0.062 -0.854841   \n",
      "1524383570  841.843780   0.116  0.086815    0.06  0.162491   0.062 -0.864074   \n",
      "1524383580  841.828358   0.116  0.068748    0.06  0.159182   0.062 -0.872280   \n",
      "1524383590  841.799287   0.116  0.041924    0.06  0.158925   0.062 -0.888218   \n",
      "1524383600  841.934962   0.116  0.082253    0.06  0.160361   0.062 -0.843932   \n",
      "\n",
      "            m5_unc  \n",
      "1524383560     0.1  \n",
      "1524383570     0.1  \n",
      "1524383580     0.1  \n",
      "1524383590     0.1  \n",
      "1524383600     0.1  \n"
     ]
    }
   ],
   "source": [
    "review_df = azi_avg_df[['m1', 'm2', 'm3', 'm5']].copy()\n",
    "\n",
    "review_df['m1_unc'] = 0.116\n",
    "review_df['m2_unc'] = 0.060\n",
    "review_df['m3_unc'] = 0.062\n",
    "review_df['m5_unc'] = 0.100\n",
    "\n",
    "review_df['m1'] = review_df['m1']/61.79\n",
    "review_df['m2'] = review_df['m2']/61.79\n",
    "review_df['m3'] = review_df['m3']/61.79\n",
    "review_df['m5'] = review_df['m5']/61.79\n",
    "\n",
    "review_df = review_df.reindex(sorted(review_df.columns), axis=1)\n",
    "\n",
    "print review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.to_csv('purcell_60hr.dat', sep=' ', index_label='time', float_format='%0.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
