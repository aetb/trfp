{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import gm2\n",
    "import trfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trolley_run_station_average(corrected_df):\n",
    "    station_phi = trfp.STATION_BARCODE_PHI\n",
    "    station_edges = trfp.STATION_BARCODE_EDGES\n",
    "\n",
    "    # tr_phi is not monotonic, so sort by tr_phi\n",
    "\n",
    "    corrected_df = corrected_df.sort_values(by=['tr_phi'])\n",
    "\n",
    "    measured_phi = corrected_df['tr_phi'].values\n",
    "    measured_extent = (np.roll(measured_phi,-1)-np.roll(measured_phi,1))/2\n",
    "    measured_extent[0] = measured_extent[0]+180\n",
    "    measured_extent[-1] = measured_extent[-1]+180\n",
    "    # print np.max(measured_extent)\n",
    "\n",
    "    corrected_df['tr_extent'] = pd.Series(measured_extent, index=corrected_df.index)\n",
    "    corrected_df = corrected_df.sort_index()\n",
    "\n",
    "    # for a given station:\n",
    "    # create a mask for when trolley is in [low edge, high edge)\n",
    "    tr_baseline = np.full((72,17), np.nan)\n",
    "    fp_baseline = np.full((72,6), np.nan)\n",
    "    summed_azimuth = np.full(72, np.nan)\n",
    "    summed_pts = np.full(72, np.nan)\n",
    "    baseline_time = np.full(72, np.nan)\n",
    "\n",
    "    for st in range(72): \n",
    "        if station_edges[st+1] > station_edges[st]:\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges[st]) & (corrected_df['tr_phi'] < station_edges[st+1])\n",
    "        else:  # case where we go over the 360 deg line\n",
    "            mask = (corrected_df['tr_phi'] >= station_edges[st]) | (corrected_df['tr_phi'] < station_edges[st+1])\n",
    "\n",
    "        out_df = corrected_df[mask]\n",
    "        summed_pts[st] = out_df.shape[0]\n",
    "        summed_azimuth[st] = sum(out_df['tr_extent'].values)        \n",
    "        baseline_time[st] = sum(out_df.index.values)/summed_pts[st]\n",
    "\n",
    "        for m in range(17):\n",
    "            st_id = 'tr,m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                tr_baseline[st, m] = sum(out_df['tr_extent'].values*out_df[st_id].values)/sum(out_df['tr_extent'].values)\n",
    "            else:\n",
    "                tr_baseline[st, m] = np.nan\n",
    "        for m in range(6):\n",
    "            st_id = 'st'+str(st)+',m'+str(m+1)\n",
    "            if sum(out_df['tr_extent'].values) != 0:\n",
    "                fp_baseline[st, m] = np.mean(out_df[st_id])\n",
    "            else:\n",
    "                fp_baseline[st, m] = np.nan\n",
    "    \n",
    "    return tr_baseline, fp_baseline, baseline_time, summed_azimuth, summed_pts\n",
    "\n",
    "def remove_trolley_effect(trolley_moment_df):\n",
    "    '''DOC STRING'''\n",
    "    barcode = trfp.STATION_BARCODE_PHI\n",
    "    veto_extent = 25\n",
    "\n",
    "    trolley_effect_removed_df = trolley_moment_df.copy()\n",
    "\n",
    "    for st in range(72):\n",
    "        print '\\rRemoving trolley image from station ' + str(st) + '.',\n",
    "        for m in range(1,7):\n",
    "            st_m = 'st' + str(st) + \",m\" + str(m)\n",
    "\n",
    "            # Unwrap the fixed probe data versus trolley position\n",
    "            raw_data = trolley_moment_df[['tr_phi', st_m]].copy()\n",
    "            raw_low = raw_data.copy()\n",
    "            raw_high = raw_data.copy()\n",
    "            raw_low['tr_phi'] = raw_low['tr_phi'] - 360\n",
    "            raw_high['tr_phi'] = raw_high['tr_phi'] + 360\n",
    "            unwrap_nomask_df = pd.concat([raw_low, raw_data, raw_high])\n",
    "\n",
    "            unwrap_mask_df = unwrap_nomask_df.copy()\n",
    "            veto_adjust = (veto_extent-7)/2\n",
    "            mask = ((unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust) |\n",
    "                    (unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust-360) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust-360) |\n",
    "                    (unwrap_nomask_df['tr_phi']>barcode[st]-2-veto_adjust+360) & (unwrap_nomask_df['tr_phi']<barcode[st]+5+veto_adjust+360))\n",
    "            unwrap_mask_df[st_m] = unwrap_nomask_df[st_m].mask(mask)\n",
    "            unwrap_mask_df['tr_phi'] = unwrap_nomask_df['tr_phi']\n",
    "\n",
    "            unwrap_filled_df = unwrap_mask_df.copy()\n",
    "            temp = unwrap_filled_df.rolling(int(500),win_type='triang',min_periods=1,center=True).mean()\n",
    "            temp = temp.rolling(int(500),win_type='triang',min_periods=1,center=True).mean()\n",
    "            unwrap_filled_df[st_m] = unwrap_filled_df[st_m].mask(mask, temp[st_m])\n",
    "\n",
    "            length = raw_data.shape[0]\n",
    "            filled_df = unwrap_filled_df.iloc[length:2*length,:]\n",
    "\n",
    "            trolley_effect_removed_df[st_m] = filled_df[st_m]\n",
    "\n",
    "    print '\\rFinished removing trolley images from ' + str(length) + ' events.'\n",
    "    return trolley_effect_removed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_phi = trfp.STATION_BARCODE_PHI\n",
    "station_edges = trfp.STATION_BARCODE_EDGES\n",
    "\n",
    "corrected_df_1 = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3956_moment_df'))\n",
    "tr_baseline_1, fp_baseline_1, baseline_time_1, summed_azimuth_1, _ = trolley_run_station_average(corrected_df_1)\n",
    "\n",
    "corrected_df_2 = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_2.h5', key='run_3997_moment_df'))\n",
    "tr_baseline_2, fp_baseline_2, baseline_time_2, summed_azimuth_2, _ = trolley_run_station_average(corrected_df_2)\n",
    "\n",
    "# load all fixed probe runs moment_df into one big moment_df (runs 3959--3994)\n",
    "print 'Appending fixed probe runs.'\n",
    "fp_moment_df = pd.read_hdf('60hr_fixed_probe_runs_new.h5', key='run_3959_moment_df')\n",
    "pts = fp_moment_df.shape[0]\n",
    "for run in np.arange(3959, 3995):\n",
    "    temp_df = pd.read_hdf('60hr_fixed_probe_runs_new.h5', key='run_'+str(run)+'_moment_df')\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    fp_moment_df = fp_moment_df.append(temp_df)\n",
    "\n",
    "# load all the trolley runs corrected_df into on big file (to show window of 60hr set) ()\n",
    "print '\\nAppending trolley runs.'\n",
    "tr_corrected_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_3955_moment_df'))\n",
    "for run in np.arange(3956, 3959):\n",
    "    temp_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_1.h5', key='run_'+str(run)+'_moment_df'))\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "for run in np.arange(3995, 3999):\n",
    "    temp_df = remove_trolley_effect(pd.read_hdf('60hr_trolley_runs_2.h5', key='run_'+str(run)+'_moment_df'))\n",
    "    pts = pts + temp_df.shape[0]\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    tr_corrected_df = tr_corrected_df.append(temp_df)\n",
    "\n",
    "print '\\nDone appending runs.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply baseline corrections, generate virtual trolley measurements\n",
    "\n",
    "fp_moment_baseline = fp_moment_df.copy()\n",
    "fp_moment_tr_run_baseline = tr_corrected_df.copy()\n",
    "\n",
    "# apply baseline corrections to each fp stm\n",
    "print \"\\nSubtracting fixed probe baselines.\"\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    for m in np.arange(num_probes):\n",
    "        stm = 'st'+str(st)+',m'+str(m+1)\n",
    "        \n",
    "        def backwards_correction(time):\n",
    "            c1 = fp_baseline_1[st, m]\n",
    "            c2 = fp_baseline_2[st, m]\n",
    "            t1 = baseline_time_1[st]\n",
    "            t2 = baseline_time_2[st]\n",
    "            return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "        \n",
    "        correction = backwards_correction(fp_moment_baseline.index.values)\n",
    "        fp_moment_baseline[stm] = fp_moment_baseline[stm] - correction\n",
    "        correction = backwards_correction(fp_moment_tr_run_baseline.index.values)\n",
    "        fp_moment_tr_run_baseline[stm] = fp_moment_tr_run_baseline[stm] - correction\n",
    "        \n",
    "        print '\\rstm: ' + stm + '.',\n",
    "        \n",
    "# replace columns in vtr with Jacobian-fixed columns from fp baseline correction\n",
    "\n",
    "print \"\\n\\nApplying Jacobian.\"\n",
    "vtr_df = fp_moment_baseline.copy()\n",
    "vtr_tr_run_df = fp_moment_tr_run_baseline.copy()\n",
    "\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    if num_probes == 4:\n",
    "        num_moments = 4\n",
    "        if st == 41:\n",
    "            J = trfp.J_4_PROBE_ST41\n",
    "        elif st == 37 | st == 39:\n",
    "            J = trfp.J_4_PROBE_ST37_ST39\n",
    "        else:\n",
    "            J = trfp.J_4_PROBE\n",
    "    else:\n",
    "        num_moments = 5\n",
    "        if st < 7:\n",
    "            J = trfp.J_6_PROBE_OFFSET\n",
    "        else:\n",
    "            J = trfp.J_6_PROBE\n",
    "#         J[0,4] = 0\n",
    "\n",
    "    # run over each vtr moment:\n",
    "    for m in np.arange(num_moments):\n",
    "        vtr_stm = 'st'+str(st)+',m'+str(m+1)\n",
    "        fp_stm = ['st'+str(st)+',m'+str(fp_m+1) for fp_m in np.arange(num_moments)]\n",
    "        vtr_df[vtr_stm] = fp_moment_baseline[fp_stm].dot(J[m])\n",
    "        vtr_tr_run_df[vtr_stm] = fp_moment_tr_run_baseline[fp_stm].dot(J[m])\n",
    "        \n",
    "        print '\\rstm: ' + stm + '.',\n",
    "        \n",
    "# Add trolley baseline correction (with backwards correction)\n",
    "print \"\\n\\nAdding trolley baselines.\"\n",
    "\n",
    "for st in np.arange(72):\n",
    "    num_probes = len(trfp.STATION_PROBE_ID[st])\n",
    "    for m in np.arange(num_probes):\n",
    "        stm = 'st'+str(st)+',m'+str(m+1)\n",
    "        \n",
    "        def backwards_correction(time):\n",
    "            c1 = tr_baseline_1[st, m]\n",
    "            c2 = tr_baseline_2[st, m]\n",
    "            t1 = baseline_time_1[st]\n",
    "            t2 = baseline_time_2[st]\n",
    "            return (c2-c1)/(t2-t1)*(time-t1) + c1\n",
    "        \n",
    "        correction = backwards_correction(vtr_df.index.values)\n",
    "        vtr_df[stm] = vtr_df[stm] + correction\n",
    "        correction = backwards_correction(vtr_tr_run_df.index.values)\n",
    "        vtr_tr_run_df[stm] = vtr_tr_run_df[stm] + correction\n",
    "        \n",
    "        print '\\rstm: ' + stm + '.',\n",
    "\n",
    "print '\\n\\nDone.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin into the agreed upon bins\n",
    "\n",
    "bins = np.arange(1524384055, 1524641055, 1000)-500\n",
    "bin_centers = np.arange(1524384055, 1524640055, 1000)\n",
    "\n",
    "vtr_time_bin_df = vtr_df.groupby(pd.cut(vtr_df.index,bins)).mean()\n",
    "vtr_time_bin_df.index = bin_centers\n",
    "\n",
    "fp_time_bin_df = fp_moment_df.groupby(pd.cut(fp_moment_df.index,bins)).mean()\n",
    "fp_time_bin_df.index = bin_centers\n",
    "\n",
    "vtr_time_bin_df.head()\n",
    "test_df = vtr_time_bin_df.copy()\n",
    "\n",
    "azi_avg_df = pd.DataFrame(np.zeros((test_df.shape[0],6)),\n",
    "                         index = test_df.index,\n",
    "                         columns = ['m' + str(m) for m in np.arange(6)+1])\n",
    "\n",
    "weight = summed_azimuth_1+summed_azimuth_2\n",
    "total_weight = np.sum(weight)\n",
    "\n",
    "for m in np.arange(6):\n",
    "    stm_list = ['st'+str(st)+',m'+str(m+1) for st in np.arange(72)]\n",
    "    azi_avg_df['m'+str(m+1)] = test_df[stm_list].multiply(weight).sum(axis=1)/total_weight\n",
    "    \n",
    "azi_avg_df.head()\n",
    "\n",
    "fig, axs = plt.subplots(2,1)\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.plot(azi_avg_df.index.values, azi_avg_df['m2']/61.79, '.')\n",
    "\n",
    "fig.set_size_inches(12,8)\n",
    "fig.tight_layout()\n",
    "\n",
    "# azi_avg_df.to_csv('no_sext_correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section was trying to find the issue with the n quad\n",
    "\n",
    "Turns out that, somehow, the hdf5 files got overwritten, and most of the 4-probe m2 moments were bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.round(trfp.J_6_PROBE[1,:],2)\n",
    "print np.round(trfp.J_6_PROBE_OFFSET[1,:],2)\n",
    "print np.round(trfp.J_4_PROBE[1,:],2)\n",
    "print np.round(trfp.J_4_PROBE_ST37_ST39[1,:],2)\n",
    "print np.round(trfp.J_4_PROBE_ST41[1,:],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot every station's m2 trend vs average. Look for weird straight lines\n",
    "\n",
    "fig,axs = plt.subplots(24,3)\n",
    "\n",
    "st = 0\n",
    "for i in range(24):\n",
    "    for j in range(3):\n",
    "        plt.sca(axs[i,j])\n",
    "        plt.plot(azi_avg_df.index.values,\n",
    "                 azi_avg_df['m2'] - np.mean(azi_avg_df['m2']),\n",
    "                 '.', color='navy')\n",
    "        plt.plot(vtr_time_bin_df.index.values,\n",
    "                 vtr_time_bin_df['st'+str(st)+',m2'] - np.mean(vtr_time_bin_df['st'+str(st)+',m2']),\n",
    "                 '.', color='orange', markersize=1)\n",
    "        plt.plot(fp_time_bin_df.index.values,\n",
    "                 fp_time_bin_df['st'+str(st)+',m2'] - np.mean(fp_time_bin_df['st'+str(st)+',m2']),\n",
    "                 '.', color='green', markersize=1)\n",
    "        axs[i,j].set_title('st '+str(st)+': '+str(trfp.STATION_PROBE_NUMBER[st])+' probes')\n",
    "        st += 1\n",
    "\n",
    "fig.set_size_inches(12,96)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all fixed probe runs moment_df into one big moment_df (runs 3959--3994)\n",
    "print 'Appending fixed probe runs.'\n",
    "fp_moment_df_2 = pd.read_hdf('60hr_fixed_probe_runs_new.h5', key='run_3959_moment_df')\n",
    "for run in np.arange(3960, 3995):\n",
    "    temp_df = pd.read_hdf('60hr_fixed_probe_runs_new.h5', key='run_'+str(run)+'_moment_df')\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    fp_moment_df_2 = fp_moment_df_2.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate 4-probe m2 moments from interp dataframes\n",
    "\n",
    "# load all fixed probe runs moment_df into one big moment_df (runs 3959--3994)\n",
    "print 'Appending fixed probe runs.'\n",
    "fp_interp_df = pd.read_hdf('60hr_fixed_probe_runs_new.h5', key='run_3959_interp_df')\n",
    "for run in range(3960, 3995):\n",
    "    temp_df = pd.read_hdf('60hr_fixed_probe_runs_new.h5', key='run_'+str(run)+'_interp_df')\n",
    "    print '\\rAppending run ' + str(run) + '.',\n",
    "    fp_interp_df = fp_interp_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 53\n",
    "\n",
    "probes = trfp.STATION_PROBE_ID[st]\n",
    "fps = ['fp'+str(probe) for probe in probes]\n",
    "stm = 'st'+str(st)+',m2'\n",
    "\n",
    "bins = np.arange(1524384055, 1524641055, 1000)-500\n",
    "bin_centers = np.arange(1524384055, 1524640055, 1000)\n",
    "\n",
    "interp_time_bin_df = fp_interp_df.groupby(pd.cut(fp_interp_df.index,bins)).mean()\n",
    "interp_time_bin_df.index = bin_centers\n",
    "\n",
    "moment_time_bin_df = fp_moment_df_2.groupby(pd.cut(fp_moment_df_2.index,bins)).mean()\n",
    "moment_time_bin_df.index = bin_centers\n",
    "\n",
    "m2_test =  interp_time_bin_df[fps].dot(np.transpose(trfp.THETA_FP_4))[1]\n",
    "\n",
    "fig,axs = plt.subplots(2,1)\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.plot(m2_test, '.', color='navy')\n",
    "plt.plot(moment_time_bin_df[stm], '.', color='orange')\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.plot(m2_test - moment_time_bin_df[stm], '.', color='navy')\n",
    "\n",
    "fig.set_size_inches(12,8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
